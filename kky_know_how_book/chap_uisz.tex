\chapter{Umělá inteligence [UISZ]}

\section{Učící se systémy a klasifikátory [USK]}

\begin{table}[H]
\centering
\begin{tabular}{p{4cm} p{12cm}}
\textit{vyučující:}             & Prof. Ing. Josef Psutka, CSc. \\
\textit{ročník/semestr studia:} & 3.ročník/LS \\
\textit{datum zkoušky:}         & X. 4. 2014 \\
\textit{hodnocení:}             & 1 \\
\textit{cíl předmětu (STAG):}   & \\
\multicolumn{2}{p{16cm}}{Cílem předmětu je seznámit studenty se základními metodami klasifikace předmětů a jevů, které jsou reprezentovány svými obrazy (vektory příznaků). Výuka bude zaměřena na klasifikátory, které jsou trénovány s podporou učitele (supervised) anebo bez učitele (unsupervised).}
\end{tabular}
\end{table}

\subsection{Kritérium minimální chyby.}
Často nejsme schopni posoudit jednoznačně, do které třídy vektor příznaků $ X $ patří. Cílem je potom nastavit klasifikátor tak, aby ztráty způsobené chybným rozhodnutím byly minimální.

\begin{definition}
Ztráta, která vznikne, jestliže obraz náležející do třídy $ \omega_s $ zařadí klasifikátor do třídy $ \omega_r $: $ l(\omega_r | \omega_s) $
\end{definition}

\begin{itemize}
\item předp., že obrazový prostor $ X $ obsahuje obrazy z $ R $ tříd: $ \omega_1, ..., \omega_R $
\item apriorní ppsti výskytu obrazů náležejících ke třídě $ \omega_r => p(\omega_r), \qquad r = 1,...,R $
\item podmíněná hustota ppsti obrazu $ x $ ze třídy $ \omega_r $ je $ p(x | \omega_r), \qquad r = 1,...,R $
\item nechť je dána matice ztrátových funkcí:
\begin{equation}
l = \begin{bmatrix} l(\omega_1 | \omega_1) & \dots & l(\omega_1 | \omega_R) \\  
\vdots & \ddots & \vdots \\
l(\omega_R | \omega_1) & \dots & l(\omega_R | \omega_R) \end{bmatrix}
\end{equation}
\end{itemize}

Předpokládejme, že na vstup klasifikátoru přicházejí $ x $ pouze z $ \omega_s $ a klasifikátor je bude zařazovat do $ \omega_r $ podle diskriminační funkce $ \omega_r = d (x, q) $.

\begin{definition}
Podmíněná střední ztráta (střední ztráta podmíněná výběrem obrazů výlučně ze třídy $ \omega_s $:
\begin{equation}
J(q | \omega_s) = \displaystyle{\int_X} l[d(x,q) | \omega_s] \cdot p(x | \omega_s) \, dx
\end{equation}
\end{definition}

Protože jednotlivé třídy $ \omega_s $ se vyskytují s ppstí $ p(\omega_s) $, bude celková střední ztráta:

\begin{equation}
J(q) = \displaystyle{\sum_{s=1}^R} J(q | \omega_s) \cdot p(\omega_s) = \displaystyle{\int_X \sum_{s=1}^R} l[d(x,q) | \omega_s] \cdot p(x | \omega_s) \cdot p(\omega_s) \, dx
\end{equation}

Hledáme $ q^* $, které minimalizuje $ J(q) $:
\begin{align}
\begin{split}
J(q^*) &= \underset{q}{\mathrm{min}} \, J(q) = \displaystyle{\int_X} \underset{q}{\mathrm{min}} \displaystyle{\sum_{s=1}^R} l[d(x,q) | \omega_s] \cdot p(x | \omega_s) \cdot p(\omega_s) \, dx = \\ &= \displaystyle{\int_X} \underset{r}{\mathrm{min}} \displaystyle{\sum_{r=1}^R} l(\omega_r | \omega_s) \cdot p(x | \omega_s) \cdot p(\omega_s) \, dx = \displaystyle{\int_X} \underset{r}{\mathrm{min}} \, L_x(\omega_r) \, dx
\end{split}
\end{align}

Místo minima $ J(q) $ hledáme minimum $ L_x (\omega_r) = \displaystyle{\sum_{r=1}^R} l(\omega_r | \omega_s) \cdot p(x | \omega_s) \cdot p(\omega_s), \qquad r = 1,...,R $.

Při klasifikaci podle funkce $ L_x (\omega_r) $ by se postupovalo tak, že pro daný $ x $ by se vyčíslily všechny $ L_x(\omega_r), r = 1,...,R $ a obraz $ x $ by se přiřadil do té třídy $ \omega_s $, pro kterou by byla ztráta minimální. Je zřejmé, že různou volbou ztrátové funkce $ l(\omega_r | \omega_s) $ dostáváme různý tvar rozhodovacího pravidla. Předpokládejme, že ztrátová funkce je zvolena tak, že při správném rozhodnutí přiřadí ztrátu $ 0 $ a při jakémkoliv špatném rozhodnutí ztrátu $ 1 $ (penalta $ 0/1 $).

\begin{equation}
l(\omega_r | \omega_s) = 1 - \delta_{rs}, \qquad \delta_{rs} = \begin{cases} 1 & r=s \\ 0 & r \neq s \end{cases}
\end{equation}

Po dosazení:
\begin{align}
\begin{split}
L_x(\omega_r) &= \displaystyle{\sum_{s=1}^R} (1-\delta_{rs}) p(x|\omega_s) \cdot p(\omega_s) = \displaystyle{\sum_{s=1}^R} p(x|\omega_s) \cdot p(\omega_s) - \displaystyle{\sum_{s=1}^R} \delta_{rs} \, p(x|\omega_s) \cdot p(\omega_s) \\ &= \displaystyle{\sum_{s=1}^R} \, \left[p(x|\omega_s) \cdot p(\omega_s)\right] - p(x|\omega_r) \cdot p(\omega_r)
\end{split}
\end{align}
Platí známý Bayesův vztah:
\begin{align}
\Aboxed{p(\omega_s|x) = \frac{p(x|\omega_s) \cdot p(\omega_s)}{p(x)}} \qquad ,
\end{align}
kde $ p(\omega_s|x) $ je aposteriorní pravděpodobnost, která vyjadřuje ppst třídy $ \omega_s $ za předpokladu, že je na vstupu klasifikátoru obraz $ x $.

\begin{description}[leftmargin=!, labelwidth=\widthof{$ p(x|\omega_s) $}]
\item[$ p(x|\omega_s) $] ... ppst $ x $ za předpokladu, že patří do $ \omega_s $
\item[$ p(\omega_s) $] ... apriorní ppst třídy $ \omega_s $
\item[$ p(x) $] ... ppst obrazu $ x $ (celková hustota funkce do obrazového prostoru) 
\end{description}

\begin{equation}
\displaystyle{\sum_{s=1}^R} p(\omega_s|x) \overset{!}{=} 1 = \displaystyle{\sum_{s=1}^R} \frac{p(x|\omega_s) \cdot p(\omega_s)}{p(x)} => p(x) = \displaystyle{\sum_{s=1}^R} p(x|\omega_s) \cdot p(\omega_s)
\end{equation}

Dosadíme: $ L_x(\omega_r) = p(x) - p(x|\omega_r) \cdot p(\omega_r) $. Hodnota $ p(x) $ je pro všechny třídy konstantní a jedná se v podstatě o aditivní konstantu, takže lze definovat novou funkci $ L'_x(\omega_r) = p(x|\omega_r) \cdot p(\omega_r) $. Klasifikace zde probíhá tak, že se hledá takové zařazení $ \omega_s $, pro které je $ L'_x(\omega_r) $ maximální:

\begin{equation}
\omega_r^* = \underset{r}{\mathrm{argmax}} \, p(x|\omega_r) \cdot p(\omega_r), \qquad r=1,...,R
\end{equation}

\subsection{Pravděpodobnostní diskriminační funkce. Souvislost s klasifikátory podle lineární diskriminační funkce, podle nejmenší vzdálenosti, podle nejbližšího souseda a podle k-nejbližšího souseda.}
Kritérium minimální chyby se často označuje jako Bayesovo kritérium. Klasifikaci lze zajistit s využitím diskriminačních funkcí:

\begin{equation}
g_r'(x) = p(x|\omega_r) \cdot p(\omega_r), \qquad r=1,...,R
\end{equation}

Klasifikátor pracující podle Bayesova kritéria se nazývá Bayesův klasifikátor. Pro jeho konstrukci je třeba znát hodnoty apriorní pravděpodobnosti a hustoty pravděpodobnosti pro každou třídu. Rozhodnutí, do které třídy neznámý obraz $ x $ patří, se provede podle hodnoty $ g_r'(x) $ výběrem maxima.

Velmi často se místo diskriminační funkce $ g_r'(x) $ používá její přirozený logaritmus:

\begin{equation}
g_r(x) = ln \, g_r'(x) = ln \, p(x|\omega_r) + ln \, p(\omega_r), \qquad r=1,...,R
\end{equation}

Např. pro $ R = 3 $ a jednosložkový vektor $ x $:

\vspace{3cm}

Předpokládejme, že obrazy v jednotlivých třídách vyhovují normálnímu rozložení (velmi častý případ). Pro obrazy v r-té třídě nechť platí:
\begin{align}
\Aboxed{p(x|\omega_r) = \frac{1}{(2\pi)^{\frac{n}{2}} \cdot \sqrt{det \, C_r}} \cdot e^{-\frac{1}{2} \cdot (x-\mu_r)^T \cdot C_r^{-1} \cdot (x-\mu_r)}}
\end{align}
\begin{description}
\item[$ \mu_r = E\{x\}_{x \in \omega_r} $] ... vektor středních hodnot obrazů r-té třídy
\item[$ C_r = E\{(x-\mu_r) \cdot (x-\mu_r)^T\}_{x \in \omega_r} $] ... kovarianční matice r-té třídy
\end{description}

Dosadíme do diskriminační funkce:
\begin{equation}
g_r(x) = ln \, p(x|\omega_r) + ln \, p(\omega_r) = - \frac{n}{2} \, ln \, 2\pi - \frac{1}{2} \, ln(det \, C_r) -\frac{1}{2} \cdot (x-\mu_r)^T \cdot C_r^{-1} \cdot (x-\mu_r) + ln \, p(\omega_r)
\end{equation}

Podle tvarů kovariančních matic $ C_r $, hodnot $ \mu_r $ a $ p(\omega_r) $ dostáváme typické tvary diskriminačních funkcí.

\begin{enumerate}
\item \textbf{Obecné kovarianční matice $ C_r, r=1,...,R $}

Hyperplochy konstantních hodnot diskriminačních funkcí $ g_r(x) $ jsou n-dimenzionální hyperelipsoidy (různě natočené a různě velké). Rozdělující hyperplocha mezi třídou $ \omega_r $ a $ \omega_s $, tj. plocha, která je geometrickým místem shodných hodnot diskriminačních funkcí $ g_r(x) $ a $ g_s(x) $.
\begin{align}
\begin{split}
\varphi_{rs}(x) &= g_r(x) - g_s(x) \overset{!}{=} 0 \\
&= -\frac{1}{2} \, ln\left[\frac{det \, C_s}{det \, C_r}\right] + ln\left[\frac{p(\omega_r)}{p(\omega_s)}\right] -\frac{1}{2} \cdot (x-\mu_r)^T \cdot C_r^{-1} \cdot (x-\mu_r) + \frac{1}{2} \cdot (x-\mu_s)^T \cdot C_s^{-1} \cdot (x-\mu_s)
\end{split}
\end{align}
Rozdělující hyperplochy mohou být podle tvaru $ C_r $ a $ C_s $ např. n-dimenzionální hyperroviny, hyperelipsoidy, hyperparaboloidy apod.

\item \textbf{Všechny třídy mají stejnou kovarianční matici $ C_r = C, \forall r=1,...,R $}

Shluky vzorků všech tříd vytvářejí stejně orientované a velké n-dimenzionální elipsoidy.
\begin{equation}
g_r(x) = -\frac{1}{2} x^T C_r^{-1} x + \mu_r^T C_r^{-1} x - \frac{1}{2} \mu_r^T C_r^{-1} \mu_r + ln \, p(\omega_r) - \frac{n}{2} \, ln \, 2\pi - \frac{1}{2} \, ln(det \, C)  
\end{equation}
Plochy konstantních velikostí diskriminačních funkcí jsou n-rozměrné elipsoidy, které mají stejný tvar a jsou stejně orientovány. Rozdělující plochu $ \varphi_{rs}(x) $ mezi třídami $ \omega_r $ a $ \omega_s $ lze vyjádřit:
\begin{align}
\begin{split}
\varphi_{rs}(x) &= (\mu_r - \mu_s)^T C^{-1} x - \frac{1}{2} \mu_r^T C^{-1} \mu_r + \frac{1}{2} \mu_s^T C^{-1} \mu_s + ln \, p(\omega_r) - ln \, p(\omega_s) = \\
&= \varphi_{rsn} x_n + \dots + \varphi_{rs1} x_1 + \varphi_{rs0} = \varphi_{rs}^T x + \varphi_{rs0}
\end{split}
\end{align}
Rozdělující plocha mezi třídami $ \omega_r $ a $ \omega_s $ je n-dimenzionální \textit{rovina}. Jedná se tedy o n-dimenzionální \textit{lineární diskriminační funkci}.

\item \textbf{Všechny třídy mají stejnou diagonální kov. matici $ C_r = C = \delta^2 I, r=1,...,R $}

Předpokladem je, že obrazy každé třídy mají statisticky nezávislé příznaky a každý příznak má stejnou varianci $ \delta^2 $. Geometricky to odpovídá situaci, kdy vzorky každé třídy vytváří shluky tvaru n-dimenzionálních koulí centrovaných kolem příslušné střední hodnoty. Potom $ det \, C = \delta^{2n} $ a $ C^{-1} = \frac{1}{\delta^2} I $. Předpokládejme, že všechny třídy jsou stejně pravděpodobné, tj. $ p(\omega_r) = p(\omega), \forall r=1,...,R $. Potom:
\begin{equation}
g_r(x) = - \frac{1}{2\delta^2} || x-\mu_r ||^2 + ln \, p(\omega) -\frac{n}{2} \, ln \, 2\pi - \frac{1}{2} \, ln(\delta^{2n}) = -k_1 \cdot || x-\mu_r ||^2 + k_2
\end{equation}
Konstanty $ k_1 > 0 $ a $ k_2 $ jsou shodné pro všechny třídy a výraz $ || x-\mu_r ||^2 $ představuje kvadrát Euklidovské vzdálenosti mezi vektorem $ x $ a střední hodnotou $ \omega_r $. Klasifikátor zařadí neznámý obraz $ x $ do té třídy $ \omega_r $, pro kterou je $ g_r(x) $ maximální. Z výrazu pro $ g_r(x) $ vyplývá, že  $ g_r(x) $ bude tím větší, čím bude $ || x-\mu_r ||^2 $ menší ($ k_1 > 0 $). Jedná se tedy v podstatě o \textit{klasifikátor podle minimální vzdálenosti}.
\end{enumerate}

\subsubsection*{Klasifikace podle minimální vzdálenosti}
Diskriminační funkce: $ g_r^*(x) = || x-\mu_r ||^2 $. Klasifikátor zařadí neznámý obraz $ x $ do té třídy, pro kterou bude $ g_r^*(x) $ minimální ($ \omega_r^* = \underset{r}{\mathrm{min}} \, d^2(x, \mu_r) $). Není to určitě nejlepší klasifikátor, ale je tu velká lákavost ho používat, protože stačí jediný obraz na třídu. Rozdělující nadrovina má tvar:
\begin{align}
\begin{split}
\varphi_{rs}(x) &= -k1 \cdot || x-\mu_r ||^2 + k_2 - \left[-k_1 \cdot || x-\mu_s ||^2 + k_2 \right] = \\
&= k_1 \cdot \left[x^Tx - 2\mu_sx + \mu_s^T\mu_s - x^Tx - 2\mu_rx + \mu_r^T\mu_r\right] \overset{!}{=} 0 \\
&=> (\mu_r - \mu_s)^T x - \frac{1}{2}(\mu_r^T\mu_r - \mu_s^T\mu_s) = 0
\end{split}
\end{align}
Rozdělující nadplochy mezi třídami jsou \textit{lineární}, jsou to n-dimenzionální roviny kolmé na úsečku $ \mu_r - \mu_s $, kterou půlí \footnote{Klasifikátor podle minimální vzdálenosti je ekvivalentní co do struktury lineárnímu klasifikátoru s $ R $ diskriminačními funkcemi, který může vytvořit až $ \frac{R (R-1)}{2} $ rozdělujících nadrovin. Má však obecně jiné parametry, tj. klasifikuje obecně jiným způsobem než lineární klasifikátor.}. Tento klasifikátor je velmi jednoduchý na implementaci - pro jeho nastavení stačí získat střední hodnoty každé třídy a pro neznámý obraz $ x $ ve fázi klasifikace vypočítat vzdálenost ke všem středním hodnotám (též nazýván \textit{klasifikátor se vzorovými etalony}. Pro svou jednoduchost je často nasazován i v případech, kdy není zabezpečena jeho optimální funkce podle kritéria minimální chyby (např. je málo početná trénovací množina nebo není znám typ rozložení nebo není známa disperzní matice ap.). To vede k negativním vlivům klasifikátoru:
\begin{itemize}
\item vzhledem k tomu, že využívá pouze střední hodnoty, nerespektuje tvar shluků jednotlivých tříd (pokud je odlišný od $ C_r = C = \delta^2 I $; tvar shluku koresponduje s tvarem disperzní matice).
\item nerespektuje případné odlišné apriorní pravděpodobnosti jednotlivých tříd
\end{itemize}
Dobrých výsledků dosáhneme, když budou třídy dobře distribuované (střední hodnoty dostatečně vzdálené, shluky dostatečně kompaktní a jednotlivé třídy stejně pravděpodobné).

\subsubsection*{Klasifikace podle nejbližšího souseda (Nearest Neighbour Classifier)}
Uvedené nevýhody klasifikátoru podle minimální vzdálenosti lze zmírnit často tím, že využijeme více vzorových etalonů pro každou třídu. Zvolíme-li pro každou třídu $ \omega_r $ $ S_r $ vzorových etalonů: $ \mu_{r1}, \mu_{r2}, \dots, \mu_{rS_r} $ , pak klasifikace probíhá podle pravidla vyjádřeného vztahem:
\begin{equation}
\omega_r^* = \underset{s,r}{\mathrm{argmin}} \, ||x-\mu{rs}|| = \underset{s,r}{\mathrm{argmin}} \, d(x, \mu{rs})
\end{equation}
Obraz $ x $ se tedy zařadí do té třídy $ \omega_r $, jejíž některý etalon má mezi všemi ostatními etalony nejmenší vzdálenost od $ x $. Tento způsob klasifikace má tu výhodu, že při dostatečně rozsáhlé trénovací množině se tvar rozdělujících funkcí pro jednotlivé třídy "blíží" Bayesovskému klasifikátoru. Na druhou stranu to však znamená značné zvýšení výpočetních nároků (klasifikátor si musí neustále pamatovat celou množinu vzorových etalonů - celou trénovací množinu) a při klasifikaci musíme počítat vzdálenost neznámého obrazu $ x $ ke všem vzorům.

\subsubsection*{Klasifikace podle k-nejbližších sousedů (k-Nearest Neighbour Classifier)}
Lepších výsledků lze často dosáhnout využitím tzv. rozhodovacího pravidla, kdy nejprve vyčíslíme všechny vzdálenosti $ ||x - \mu_{rs}|| \forall r=1,...,R \forall s=1,...,S_r $ a pak je pro každou třídu $ \omega_r $ uspořádáme tak, aby pro nový soubor $ ||x - \mu_{r[s]}|| $ platilo:
\begin{align*}
||x - \mu_{r[1]}|| \leq ||x - \mu_{r[2]}|| \leq \dots \leq ||x - \mu_{r[S_r]}||
\end{align*}
Klasifikátor pak zařadí obraz $ x $ do třídy $ \omega_r^* $ podle minima průměrné vzdálenosti k-nejbližších sousedů:
\begin{equation}
\omega_r^* = \underset{r}{\mathrm{argmin}} \, \frac{1}{k} \displaystyle{\sum_{k=1}^k} ||x - \mu_{r[i]}||, \qquad r=1,...,R
\end{equation}
Nevýhody:
\begin{itemize}
\item musím si pamatovat všechny obrazy
\item při každé klasifikaci náročné výpočty
\end{itemize}

\subsection{Klasifikátor s lineární diskriminační funkcí. Klasifikace do dvou a do více tříd.}
Pokud obrazy v jednotlivých třídách podléhají normálnímu rozložení a všechny třídy vykazují stejnou kovarianční matici $ C $, je optimální nastavení klasifikátoru podle kritéria minimální chyby (Bayesova kritéria) zabezpečeno \textit{lineárními diskriminačními funkcemi}. Vzhledem k jejich výhodným analytickým vlastnostem se jich ovšem využívá i v případech, kdy výše uvedené podmínky splněny nejsou a nebo, a to je častější případ, kdy ověření platnosti těchto podmínek je nepřiměřeně náročné (např. nelze statisticky prokázat typ rozložení vzhledem k malému počtu obrazů ap.). Zvolíme-li v takovém případě rozhodovací pravidlo založené na lineárních diskriminačních funkcích, musíme mít vždy na paměti, že jsme nezvolili optimální řešení s hlediska Bayesova kriteria minimální chyby. Přesto je třeba říci, že v případech, kdy obrazy jednotlivých tříd jsou dobře distribuované, tj. vytvářejí kompaktní shluky, které jsou od sebe dostatečně vzdálené (lineárně separabilní třídy) toto zjednodušení dostatečně vyhovuje.

Uvažme lineární diskriminační funkci $ g(x) = q_0 + q_1 x_1 + \dots q_n x_n = q_0 + \displaystyle{\sum_{i=1}^{n}} q_i x_i $, kde $ q_i $ jsou váhy funkce a $ q_0 $ je práh funkce.

Dále mějme $ ||q|| = \sqrt{q_1^2 + q_2^2 + \dots + q_n^2} $. Pro $ n=2 $:
\vspace{3cm}

\subsubsection*{Klasifikace do dvou tříd (dichotomie)}
Při klasifikaci do dvou tříd $ \omega_1 $ a $ \omega_2 $ stačí k rozhodnutí jediná diskriminační funkce:
\begin{equation}
g(x) = q_0 + \displaystyle{\sum_{i=1}^{n}} q_i x_i
\end{equation}
Pro $ g(x) > 0 $ je $ x \in \omega_1 $, pro $ g(x) < 0 $ je $ x \in \omega_2 $.

\subsubsection*{Klasifikace do více tříd}
\begin{enumerate}[label=(\alph*)]
\item Předpokládejme, že obrazy každé třídy jsou \textit{lineárně separovatelné} od obrazů všech ostatních tříd. Pak diskriminační funkce mezi třídami $ \omega_r $ a $ \bar{\omega_r} $ je:
\begin{equation}
g_r(x) = q_{r,0} + \displaystyle{\sum_{i=1}^{n}} q_{r,i} x_i
\end{equation}
a platí, že pro $ x \in \omega_r $ je $ g_r(x) > 0 $ a pro $ x \in \bar{\omega_r} $ je $ g_r(x) < 0 $. Klasifikátor pak rozhodne o zařazení $ x $ do té třídy $ \omega_r (r=1,...,R) $ pro níž je diskriminační funkce $ g_r(x) > 0 $. Problém je však v tom, že se může stát, že pro neznámé $ x $ bude hodnota více než jedné diskriminační funkce větší než $ 0 $. V takovém případě klasifikátor není schopen rozhodnout \footnote{Tento způsob klasifikace má jistá omezení, např. v prostoru dimenze $ n = 2 $ lze takto rozdělit maximálně $ R =3 $ dobře distribuované třídy.}.

Př. ($ n=2 $, $ R=3 $):
\vspace{4cm}

\item Předpokládejme, že obrazy každé třídy jsou \textit{po dvojicích lineárně separovatelné} od všech ostatních tříd. V tomto případě existuje celkově $ \frac{R(R-1)}{2} $ diskriminačních funkcí $ \varphi_{rs}(x); r,s = 1,...,R \land r \neq s $, které vytvářejí rozdělující roviny mezi obrazy všech dvojic tříd. Pro obraz $ x \in \omega_r $ pak platí $ \varphi_{rs} (x) > 0 \, \forall s \neq r $, viz\footnote{Samozřejmě platí $ \varphi_{rs}(x) = -\varphi_{sr}(x) $. V mnoha případech se nevyužívá všech $ \frac{R(R-1)}{2} $ diskriminačních funkcí. V tomto případě se opět objevují oblasti, pro které nejsme schopni rozhodnout a zařazení $ x $.}.

Př. ($ n=2 $, $ R=4 $):
\vspace{4cm}

\item Předpokládejme\footnote{Vylepšení ad a).}, že existují diskriminační funkce $ g_r(x), r=1,...,R $ z případu ad a). Vytvoříme rozdělující hyperplochy mezi třídami $ r $ a $ s $.
\begin{equation}
\varphi_{rs}(x) = g_r(x) - g_s(x) \overset{!}{=} 0
\end{equation}
Pro $ \varphi_{rs}(x) > 0 $ je $ g_r(x) > g_s(x) $. Z toho vyplývá, že klasifikátor zařadí $ x $ do $ \omega_r $, jestliže $ g_r(x) > g_s(x) \, \forall s=1,...,R; \, s \neq r $. Viz poznámky\footnote{Rozdělující funkce $ \varphi_{rs}(x) $ rozdělují obrazový prostor bezezbytku (nejsou hluché oblasti, kde není možno provést přiřazení).}.
\vspace{4cm}
\end{enumerate}

\textit{Hodnocení:} Je zřejmé, že případ ad a) není vhodný k aplikování vzhledem k vytváření rozsáhlých oblastí, ve kterých nejsme schopni provést jednoznačné přiřazení. Rozhodnutí mezi případem ad b) a ad c) závisí do značné míry na intuici (zvláště v prostorech vyšší dimenze). Obecně lze říci, že případ ad b) vyžaduje určení $ \frac{R(R-1)}{2} $ diskriminačních funkcí $ \varphi_{rs}(x) $, kdežto případ ad c) požaduje nalezení pouze $ R $ diskriminačních funkcí $ g_r(x) $. Jestliže se však počet tříd $ R $ blíží dimenzi $ n $ obrazového prostoru nebo se očekává, že obrazy jednotlivých tříd jsou špatně distribuované, bude možná postup podle ad b) lepším řešením.

\subsection{Metody nastavování klasifikátorů (trénování klasifikátorů).}
\begin{enumerate}[label=(\alph*)]
\item Známe-li celou trénovací množinu apriori, můžeme se pokusit o \textit{analytické řešení:}
\begin{itemize}
\item \textit{pravděpodobnostní diskriminační funkce}: je třeba určit a prokázat typ rozložení a hodnoty parametrů rozložení včetně apriorní pravděpodobnosti tříd
\item \textit{klasifikátor dle minima vzdálenosti}: je třeba určit střední hodnotu obrazů v každé třídě
\item \textit{klasifikátor podle nejbližšího či k-nejbližšího souseda}
\end{itemize}
Pro prostory vyšší dimenze nepoužitelné, navíc je třeba si pamatovat celou trénovací množinu. Dále není umožněno dotrénování klasifikátoru, pokud se objeví nové informace o trénovací množině. Častou chybou je nerespektování apriorních pravděpodobností tříd.
\item Trénovací množinu neznáme apriori: \textit{metody učení}
Učící se klasifikátor má dvě fáze:
\begin{itemize}
\item \textit{fáze učení}: postupně předkládány dvojice $ [x_k, \Omega_k] $ z trénovací množiny, nastavujeme parametry $ q $ tak, aby pro $ k \to \infty $ bude $ q \to q^* $ (optimální nastavení, minimální střední hodnota ztrát).
\item \textit{fáze klasifikace}: využívá se zkušenosti nashromážděné v parametrech $ q $ k predikci neznámých obrazů. Klasifikátor se chová jako jednoúčelový automat.
\end{itemize}
Střední ztráta:
\begin{equation}
J(q) = \displaystyle{\int_{X \times O}} Q(x, \Omega, q) \, dF(x, \Omega) = \displaystyle{\sum_{r=1}^R} \, p(\omega_r) \displaystyle{\int_{X}} Q(x, \Omega, q) p(x|\omega_r) \, dx
\end{equation}
Úkolem je najít takové $ q^* $, které minimalizuje $ J(q) $.
\begin{equation}
\underset{q}{\mathrm{grad}} \, J(q^*) = \displaystyle{\int_{X \times O}} \underset{q}{\mathrm{grad}} \, Q(x, \Omega, q^*) \, dF(x, \Omega) \overset{!}{=} 0
\end{equation}
Běžně ovšem neznáme distribuční (ani hustotní) funkce, proto se obracíme na rekurentní algoritmy, které obcházejí přímé řešení rovnice. Existují dva přístupy:
\begin{itemize}
\item metody učení založené na odhadování hustot ppsti
\item metody učení založené na přímé minimalizaci ztrát
\end{itemize}
\end{enumerate}

\subsubsection*{Metody učení založené na odhadování hustot ppsti}
Snaha stanovit distribuční funkci $ dF(x, \Omega) $ z trénovací množiny, poté se využije kritérium minimální chyby a dostáváme soustavu diskriminačních funkcí $ g_r(x) = p(x|\omega_r) \cdot p(\omega_r) $. Hledáme odhady $ \hat{p}(x|\omega_r) $ a $ \hat{p}(\omega_r) $. Žádané vlastnosti:
\begin{itemize}
\item \textit{nestrannost}: má zaručovat, že se odhad bude v průměru pohybovat kolem neznámé odhadované veličiny
\item \textit{konzistence}: s rostoucím počtem obrazů trénovací množiny se odhad blíží stále více k odhadované veličině
\item \textit{eficience}: eficientní odhad je odhad s nejmenší disperzí
\end{itemize}

Je-li trénovací množina vybrána nezávisle, lze odhad $ \hat{p}(\omega_r) $ apriorní ppsti určit podle:
\begin{equation}
\hat{p}(\omega_r) = \frac{K_r}{K}, \qquad r=1,...,R
\end{equation}
kde $ K_r $ je počet obrazů trénovací množiny, které patří do třídy $ \omega_r $ a $ K $ je celkový počet obrazů v trénovací množině. Podle velikosti apriorní informace o hledané hustotě rozdělujeme metody získávání odhadů hustotní funkce na \textit{parametrické} a \textit{neparametrické}.
\begin{enumerate}
\item \textit{Parametrické metody}: známe informaci o tvaru hustotní funkce $ p(x|\omega_r) $, ale neznáme parametry $ q $, který rozložení blíže popisuje (např. u Gausse: $ \mu_r $ a $ \delta_r $).
\begin{itemize}
\item \textit{Metoda momentů}

Teoretické momenty náhodných veličin se porovnávají s výběrovými momenty do takového stupně $ l $, kolik je neznámých parametrů.

výběrový průměr: $ \bar{x} = \frac{1}{K} \displaystyle{\sum_{k=1}^K x_k} $

výběrový rozptyl: $ S = \frac{1}{K-1} \displaystyle{\sum_{k=1}^K (x_k-\bar{x})(x_k-\bar{x})^T} $

$ l $-tý výběrový moment: $ M_l = \frac{1}{K} \displaystyle{\sum_{k=1}^K x_k^l} $
\item \textit{Metoda nejlepších nestranných lineárních odhadů}

Odhad parametrů $ q $ se uvažuje jako lineární funkce obrazů $ x_1, ..., x_K $: $ \hat{q} = c_1 x_1 + ... + c_K x_K $. Koeficienty $ c_k, k=1,...,K $ se určují z podmínek nestrannosti a minimální disperze odhadu $ \hat{q} $. Platí $ E \, \hat{q} = q $ a odhad $ \hat{q} $ parametru $ q $ je eficientní, jestliže minimalizuje stopu disperzní matice $ tr \, D \, \hat{q} $.

\item \textit{Metoda maximální věrohodnosti}

Metoda je založena na maximalizaci tzv. Fisherovy funkce věrohodnosti:
\begin{equation}
L(x_1, ..., x_K|q) = \displaystyle{\prod_{k=1}^K} p(x_k|q)
\end{equation}
Hledáme takový parametr $ q $, pro který je funkce maximální. Místo tohoto maxima lze hledat maximum logaritmu této věrohodnostní funkce $ \underset{q}{\mathrm{grad}} \, ln \, L(x_1,...,x_K|q) \overset{!}{=} 0 $. Např. pro normální rozdělení je nejlepším odhadem střední hodnoty aritmetický průměr $ \hat{\mu} = \frac{1}{N} \displaystyle{\sum_{i=1}^N} x_i $ a nejlepším odhadem kovarianční matice je: 
\begin{equation}
C = \frac{1}{N} \displaystyle{\sum_{i=1}^N} (x_i-\mu)(x_i-\mu)^T
\end{equation}
\end{itemize}
\item \textit{Neparametrické metody}: máme nulovou apriorní informaci, musíme odhadovat celý tvar hustotní funkce.
\begin{itemize}
\item \textit{Metoda histogramu}

Obrazový prostor rozdělíme na $ L $ disjunktních podmnožin $ A_l, l=1,...,L $ (obvykle to jsou n-rozměrné intervaly). Symbolem $ c_l $ označíme počet obrazů $ x_k $, které padnou do $ A_l $, dělený číslem $ K $. Za odhad $ \hat{p}(x|\omega_r) $ pak bereme po částech konstantní funkci, která na intervalu $ A_l $ nabývá $ c_l $. Nevýhodou je nutnost znalosti apriorního rozdělení obrazového prostoru na intervaly před fází učení. Odhad hustotní funkce:
\begin{equation}
\hat{p}(x|\omega_r) = \displaystyle{\sum_{l=1}^L c_l \cdot \varphi_l(x)}, \qquad \varphi_l = \begin{cases} 1 & x \in A_l \\ 0 & jinak \end{cases}
\end{equation}
\end{itemize}
\end{enumerate}

\subsubsection*{Metody učení založené na přímé minimalizaci ztrát}
Cílem je navrhnout parametry klasifikátoru, které budou minimalizovat ztrátu, rekurentním vypočítáváním odhadů $ \hat{q}(0) \to \hat{q}(1) \to \dots \to \hat{q}^* $. 
\begin{equation}
\underset{q}{\mathrm{grad}} \, J(q^*) = \displaystyle{\int_{X \times O}} \underset{q}{\mathrm{grad}} \, Q(x, \Omega, q^*) \, dF(x, \Omega) = 0
\end{equation}
Nabízí se využití gradientních metod. V praktických úlohách neznáme distribuční funkci a nelze tak vyčíslit $ \mathrm{grad} \, J(q) $. Navíc, při pevném $ q $ hodnota funkce $ Q(x, \Omega, q^*) $ náhodně kolísá v závislosti na $ x $ a $ \Omega $ - to je u gradientních metod nepoužitelné. Řešení rovnice hledáme pomocí metody \textit{stochastických aproximací}. Základní algoritmus přímé minimalizace ztrát:
\begin{equation}
q(k+1) = q(k) - C_{k+1} \, \underset{q}{\mathrm{grad}} \, Q[x(k+1), \Omega(k+1), q(k)] \qquad k=1,...,K
\end{equation}
$ C_k $ je čtvercová matice (obvykle $ C_k = c_k \cdot I $), $ [x(k), \Omega(k)] $ je k-tá dvojice trénovací množiny a $ q $ je vektor parametrů. Vhodnou volbou $ Q(x, \Omega, q) $ a $ C_k $ lze získat téměř všechny algoritmy přímé minimalizace ztrát.

Položíme $ x_0 = 1 $: $ x^T = [x_0, x_1, ..., x_n] $ a váhový vektor rozšíříme o práh $ q_0 $: $ q^T = [q_0, q_1, ..., q_n] $. Diskriminační funkce má tvar $ g(x) = q^T \cdot x $ a rozhodovací pravidlo $ \omega = \mathrm{sign} \, g(x) $. Po zavedení pásma necitlivosti $ \delta $ volíme:
\begin{equation}
\underset{q}{\mathrm{grad}} \, Q(x, \Omega, q) = \begin{cases} 0 & q^T x \Omega \geq \delta \\ -x \Omega & q^T x \Omega < \delta \end{cases}
\end{equation}
Dosadíme-li do vztahu pro rekurentní výpočet $ q $, dostaneme algoritmus učení:
\begin{align}
\begin{split}
q(k+1) &= q(k) - C_{k+1} \, \underset{q}{\mathrm{grad}} \, Q[x(k+1), \Omega(k+1), q(k)] = \\
&= \begin{cases} q(k) & q^T(k) x(k+1) \Omega(k+1) \geq 0 \\ q(k) + C_{k+1} x(k+1) \Omega(k+1) & q^T(k) x(k+1) \Omega(k+1) < 0 \end{cases}
\end{split}
\end{align}
\begin{enumerate}[label=(\alph*)]
\item \textit{Rosenblattův algoritmus}

$ C_k = C_{k+1} = 1 $, $ \delta = 0 $:
\begin{align}
\begin{split}
q(k+1) = \begin{cases} q(k) & q^T(k) x(k+1) \Omega(k+1) \geq 0 \\ q(k) + x(k+1) \Omega(k+1) & q^T(k) x(k+1) \Omega(k+1) < 0 \end{cases}
\end{split}
\end{align}
Nedostatkem je, že nikdy nezjistíme absolutně přesný klasifikátor. 
\item \textit{Metoda konstantních přírůstků}

$ C_k = \frac{\beta}{||x(k)||^2}, \, \beta > 0 $
\begin{align}
\begin{split}
q(k+1) = \begin{cases} q(k) & q^T(k) x(k+1) \Omega(k+1) \geq \delta \\ q(k) + \frac{\beta}{||x(k+1)||^2} x(k+1) \Omega(k+1) & q^T(k) x(k+1) \Omega(k+1) < \delta \end{cases}
\end{split}
\end{align}
\item \textit{Upravená metoda konstantních přírůstků}

Připočítáváme vektor $ \frac{\beta}{||x(k+1)||^2} x(k+1) \Omega(k+1) $ tolikrát, až pro určitý obraz $ x(k+1) $ dosáhneme správné klasifikace, tj. bude platit $ q^T(k+1) x(k+1) \Omega(k+1) \geq \delta $.
Může se stát, že nikdy nedojdeme k řešení.
\item \textit{Relaxační metoda učení}

\begin{align}
\begin{split}
q(k+1) = \begin{cases} q(k) & q^T(k) x(k+1) \Omega(k+1) \geq \delta \\ q(k) + 2 C_{k+1} x(k+1) \Omega(k+1) [\delta - q^T(k) x(k+1) \Omega(k+1)] & q^T(k) x(k+1) \Omega(k+1) < \delta \end{cases}
\end{split}
\end{align}

kde $ c_k = \frac{\sigma}{2 ||x(k)||^2} $ a $ \sigma \in (0, 2) $.
\end{enumerate}

\subsection{Metody shlukové analýzy (učení bez učitele).}

\subsection{Výběr informativních příznaků.}

\section{Neuronové sítě [NEU]}

\begin{table}[H]
\centering
\begin{tabular}{p{4cm} p{12cm}}
\textit{vyučující:}             & Doc. Dr. Ing. Vlasta Radová \\
\textit{ročník/semestr studia:} & 5.ročník/ZS \\
\textit{datum zkoušky:}         & 5. 1. 2017 \\
\textit{hodnocení:}             & 1 \\
\textit{cíl předmětu (STAG):}   & \\
\multicolumn{2}{p{16cm}}{Cílem předmětu je seznámit studenty se základními typy umělých neuronových sítí a s možnostmi jejich využití.}
\end{tabular}
\end{table}

\subsection{Základní umělé modely neuronu, vlastnosti, souvislost s biologickým neuronem.}

\subsection{Základní typy neuronových sítí. Způsoby činnosti a učení neuronových sítí.}

\subsection{Algoritmus backpropagation.}

\subsection{Sítě se zpětnou vazbou. Hopfieldova neuronová síť.}

\subsection{Samoorganizující se sítě.}

\subsection{Oblasti použití neuronových sítí.}

\section{Zpracování digitalizovaného obrazu [ZDO]}

\begin{table}[H]
\centering
\begin{tabular}{p{4cm} p{12cm}}
\textit{vyučující:}             & Doc. Ing. Miloš Železný Ph.D. \\
								 & Ing. Petr Neduchal \\
\textit{ročník/semestr studia:} & 4.ročník/LS \\
\textit{datum zkoušky:}         & 13. 7. 2015 \\
\textit{hodnocení:}             & 1 \\
\textit{cíl předmětu (STAG):}   & \\
\multicolumn{2}{p{16cm}}{Porozumět principům zpracování digitalizovaného obrazu a počítačového vidění. Analyzovat vlastnosti obrazové informace a interpretovat tyto informace, navrhnout a vytvořit algoritmus pro zpracování obrazové informace s cílem rozpoznání objektů, jevů či vlastností scény v obraze obsažené.}
\end{tabular}
\end{table}

\subsection{Bodové jasové transformace.}

\subsection{Geometrické transformace.}

\subsection{Filtrace šumu.}

\subsection{Gradientní operátory.}

\subsection{Metody segmentace.}

\subsection{Matematická morfologie.}