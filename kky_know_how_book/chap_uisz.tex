\chapter{Umělá inteligence [UISZ]}

\section{Učící se systémy a klasifikátory [USK]}

\begin{table}[H]
\centering
\begin{tabular}{p{4cm} p{12cm}}
\textit{vyučující:}             & Prof. Ing. Josef Psutka, CSc. \\
\textit{ročník/semestr studia:} & 3.ročník/LS \\
\textit{datum zkoušky:}         & X. 4. 2014 \\
\textit{hodnocení:}             & 1 \\
\textit{cíl předmětu (STAG):}   & \\
\multicolumn{2}{p{16cm}}{Cílem předmětu je seznámit studenty se základními metodami klasifikace předmětů a jevů, které jsou reprezentovány svými obrazy (vektory příznaků). Výuka bude zaměřena na klasifikátory, které jsou trénovány s podporou učitele (supervised) anebo bez učitele (unsupervised).}
\end{tabular}
\end{table}

\subsection{Kritérium minimální chyby.}
Často nejsme schopni posoudit jednoznačně, do které třídy vektor příznaků $ X $ patří. Cílem je potom nastavit klasifikátor tak, aby ztráty způsobené chybným rozhodnutím byly minimální.

\begin{definition}
Ztráta, která vznikne, jestliže obraz náležející do třídy $ \omega_s $ zařadí klasifikátor do třídy $ \omega_r $: $ l(\omega_r | \omega_s) $
\end{definition}

\begin{itemize}
\item předp., že obrazový prostor $ X $ obsahuje obrazy z $ R $ tříd: $ \omega_1, ..., \omega_R $
\item apriorní ppsti výskytu obrazů náležejících ke třídě $ \omega_r => p(\omega_r), \qquad r = 1,...,R $
\item podmíněná hustota ppsti obrazu $ x $ ze třídy $ \omega_r $ je $ p(x | \omega_r), \qquad r = 1,...,R $
\item nechť je dána matice ztrátových funkcí:
\begin{equation}
l = \begin{bmatrix} l(\omega_1 | \omega_1) & \dots & l(\omega_1 | \omega_R) \\  
\vdots & \ddots & \vdots \\
l(\omega_R | \omega_1) & \dots & l(\omega_R | \omega_R) \end{bmatrix}
\end{equation}
\end{itemize}

Předpokládejme, že na vstup klasifikátoru přicházejí $ x $ pouze z $ \omega_s $ a klasifikátor je bude zařazovat do $ \omega_r $ podle diskriminační funkce $ \omega_r = d (x, q) $.

\begin{definition}
Podmíněná střední ztráta (střední ztráta podmíněná výběrem obrazů výlučně ze třídy $ \omega_s $:
\begin{equation}
J(q | \omega_s) = \displaystyle{\int_X} l[d(x,q) | \omega_s] \cdot p(x | \omega_s) \, dx
\end{equation}
\end{definition}

Protože jednotlivé třídy $ \omega_s $ se vyskytují s ppstí $ p(\omega_s) $, bude celková střední ztráta:

\begin{equation}
J(q) = \displaystyle{\sum_{s=1}^R} J(q | \omega_s) \cdot p(\omega_s) = \displaystyle{\int_X \sum_{s=1}^R} l[d(x,q) | \omega_s] \cdot p(x | \omega_s) \cdot p(\omega_s) \, dx
\end{equation}

Hledáme $ q^* $, které minimalizuje $ J(q) $:
\begin{align}
\begin{split}
J(q^*) &= \underset{q}{\mathrm{min}} \, J(q) = \displaystyle{\int_X} \underset{q}{\mathrm{min}} \displaystyle{\sum_{s=1}^R} l[d(x,q) | \omega_s] \cdot p(x | \omega_s) \cdot p(\omega_s) \, dx = \\ &= \displaystyle{\int_X} \underset{r}{\mathrm{min}} \displaystyle{\sum_{r=1}^R} l(\omega_r | \omega_s) \cdot p(x | \omega_s) \cdot p(\omega_s) \, dx = \displaystyle{\int_X} \underset{r}{\mathrm{min}} \, L_x(\omega_r) \, dx
\end{split}
\end{align}

Místo minima $ J(q) $ hledáme minimum $ L_x (\omega_r) = \displaystyle{\sum_{r=1}^R} l(\omega_r | \omega_s) \cdot p(x | \omega_s) \cdot p(\omega_s), \qquad r = 1,...,R $.

Při klasifikaci podle funkce $ L_x (\omega_r) $ by se postupovalo tak, že pro daný $ x $ by se vyčíslily všechny $ L_x(\omega_r), r = 1,...,R $ a obraz $ x $ by se přiřadil do té třídy $ \omega_s $, pro kterou by byla ztráta minimální. Je zřejmé, že různou volbou ztrátové funkce $ l(\omega_r | \omega_s) $ dostáváme různý tvar rozhodovacího pravidla. Předpokládejme, že ztrátová funkce je zvolena tak, že při správném rozhodnutí přiřadí ztrátu $ 0 $ a při jakémkoliv špatném rozhodnutí ztrátu $ 1 $ (penalta $ 0/1 $).

\begin{equation}
l(\omega_r | \omega_s) = 1 - \delta_{rs}, \qquad \delta_{rs} = \begin{cases} 1 & r=s \\ 0 & r \neq s \end{cases}
\end{equation}

Po dosazení:
\begin{align}
\begin{split}
L_x(\omega_r) &= \displaystyle{\sum_{s=1}^R} (1-\delta_{rs}) p(x|\omega_s) \cdot p(\omega_s) = \displaystyle{\sum_{s=1}^R} p(x|\omega_s) \cdot p(\omega_s) - \displaystyle{\sum_{s=1}^R} \delta_{rs} \, p(x|\omega_s) \cdot p(\omega_s) \\ &= \displaystyle{\sum_{s=1}^R} \, \left[p(x|\omega_s) \cdot p(\omega_s)\right] - p(x|\omega_r) \cdot p(\omega_r)
\end{split}
\end{align}
Platí známý Bayesův vztah:
\begin{align}
\Aboxed{p(\omega_s|x) = \frac{p(x|\omega_s) \cdot p(\omega_s)}{p(x)}} \qquad ,
\end{align}
kde $ p(\omega_s|x) $ je aposteriorní pravděpodobnost, která vyjadřuje ppst třídy $ \omega_s $ za předpokladu, že je na vstupu klasifikátoru obraz $ x $.

\begin{description}[leftmargin=!, labelwidth=\widthof{$ p(x|\omega_s) $}]
\item[$ p(x|\omega_s) $] ... ppst $ x $ za předpokladu, že patří do $ \omega_s $
\item[$ p(\omega_s) $] ... apriorní ppst třídy $ \omega_s $
\item[$ p(x) $] ... ppst obrazu $ x $ (celková hustota funkce do obrazového prostoru) 
\end{description}

\begin{equation}
\displaystyle{\sum_{s=1}^R} p(\omega_s|x) \overset{!}{=} 1 = \displaystyle{\sum_{s=1}^R} \frac{p(x|\omega_s) \cdot p(\omega_s)}{p(x)} => p(x) = \displaystyle{\sum_{s=1}^R} p(x|\omega_s) \cdot p(\omega_s)
\end{equation}

Dosadíme: $ L_x(\omega_r) = p(x) - p(x|\omega_r) \cdot p(\omega_r) $. Hodnota $ p(x) $ je pro všechny třídy konstantní a jedná se v podstatě o aditivní konstantu, takže lze definovat novou funkci $ L'_x(\omega_r) = p(x|\omega_r) \cdot p(\omega_r) $. Klasifikace zde probíhá tak, že se hledá takové zařazení $ \omega_s $, pro které je $ L'_x(\omega_r) $ maximální:

\begin{equation}
\omega_r^* = \underset{r}{\mathrm{argmax}} \, p(x|\omega_r) \cdot p(\omega_r), \qquad r=1,...,R
\end{equation}

\subsection{Pravděpodobnostní diskriminační funkce. Souvislost s klasifikátory podle lineární diskriminační funkce, podle nejmenší vzdálenosti, podle nejbližšího souseda a podle k-nejbližšího souseda.}
Kritérium minimální chyby se často označuje jako Bayesovo kritérium. Klasifikaci lze zajistit s využitím diskriminačních funkcí:

\begin{equation}
g_r'(x) = p(x|\omega_r) \cdot p(\omega_r), \qquad r=1,...,R
\end{equation}

Klasifikátor pracující podle Bayesova kritéria se nazývá Bayesův klasifikátor. Pro jeho konstrukci je třeba znát hodnoty apriorní pravděpodobnosti a hustoty pravděpodobnosti pro každou třídu. Rozhodnutí, do které třídy neznámý obraz $ x $ patří, se provede podle hodnoty $ g_r'(x) $ výběrem maxima.

Velmi často se místo diskriminační funkce $ g_r'(x) $ používá její přirozený logaritmus:

\begin{equation}
g_r(x) = ln \, g_r'(x) = ln \, p(x|\omega_r) + ln \, p(\omega_r), \qquad r=1,...,R
\end{equation}

Např. pro $ R = 3 $ a jednosložkový vektor $ x $:

\vspace{3cm}

Předpokládejme, že obrazy v jednotlivých třídách vyhovují normálnímu rozložení (velmi častý případ). Pro obrazy v r-té třídě nechť platí:
\begin{align}
\Aboxed{p(x|\omega_r) = \frac{1}{(2\pi)^{\frac{n}{2}} \cdot \sqrt{det \, C_r}} \cdot e^{-\frac{1}{2} \cdot (x-\mu_r)^T \cdot C_r^{-1} \cdot (x-\mu_r)}}
\end{align}
\begin{description}
\item[$ \mu_r = E\{x\}_{x \in \omega_r} $] ... vektor středních hodnot obrazů r-té třídy
\item[$ C_r = E\{(x-\mu_r) \cdot (x-\mu_r)^T\}_{x \in \omega_r} $] ... kovarianční matice r-té třídy
\end{description}

Dosadíme do diskriminační funkce:
\begin{equation}
g_r(x) = ln \, p(x|\omega_r) + ln \, p(\omega_r) = - \frac{n}{2} \, ln \, 2\pi - \frac{1}{2} \, ln(det \, C_r) -\frac{1}{2} \cdot (x-\mu_r)^T \cdot C_r^{-1} \cdot (x-\mu_r) + ln \, p(\omega_r)
\end{equation}

Podle tvarů kovariančních matic $ C_r $, hodnot $ \mu_r $ a $ p(\omega_r) $ dostáváme typické tvary diskriminačních funkcí.

\begin{enumerate}
\item \textbf{Obecné kovarianční matice $ C_r, r=1,...,R $}

Hyperplochy konstantních hodnot diskriminačních funkcí $ g_r(x) $ jsou n-dimenzionální hyperelipsoidy (různě natočené a různě velké). Rozdělující hyperplocha mezi třídou $ \omega_r $ a $ \omega_s $, tj. plocha, která je geometrickým místem shodných hodnot diskriminačních funkcí $ g_r(x) $ a $ g_s(x) $.
\begin{align}
\begin{split}
\varphi_{rs}(x) &= g_r(x) - g_s(x) \overset{!}{=} 0 \\
&= -\frac{1}{2} \, ln\left[\frac{det \, C_s}{det \, C_r}\right] + ln\left[\frac{p(\omega_r)}{p(\omega_s)}\right] -\frac{1}{2} \cdot (x-\mu_r)^T \cdot C_r^{-1} \cdot (x-\mu_r) + \frac{1}{2} \cdot (x-\mu_s)^T \cdot C_s^{-1} \cdot (x-\mu_s)
\end{split}
\end{align}
Rozdělující hyperplochy mohou být podle tvaru $ C_r $ a $ C_s $ např. n-dimenzionální hyperroviny, hyperelipsoidy, hyperparaboloidy apod.

\item \textbf{Všechny třídy mají stejnou kovarianční matici $ C_r = C, \forall r=1,...,R $}

Shluky vzorků všech tříd vytvářejí stejně orientované a velké n-dimenzionální elipsoidy.
\begin{equation}
g_r(x) = -\frac{1}{2} x^T C_r^{-1} x + \mu_r^T C_r^{-1} x - \frac{1}{2} \mu_r^T C_r^{-1} \mu_r + ln \, p(\omega_r) - \frac{n}{2} \, ln \, 2\pi - \frac{1}{2} \, ln(det \, C)  
\end{equation}
Plochy konstantních velikostí diskriminačních funkcí jsou n-rozměrné elipsoidy, které mají stejný tvar a jsou stejně orientovány. Rozdělující plochu $ \varphi_{rs}(x) $ mezi třídami $ \omega_r $ a $ \omega_s $ lze vyjádřit:
\begin{align}
\begin{split}
\varphi_{rs}(x) &= (\mu_r - \mu_s)^T C^{-1} x - \frac{1}{2} \mu_r^T C^{-1} \mu_r + \frac{1}{2} \mu_s^T C^{-1} \mu_s + ln \, p(\omega_r) - ln \, p(\omega_s) = \\
&= \varphi_{rsn} x_n + \dots + \varphi_{rs1} x_1 + \varphi_{rs0} = \varphi_{rs}^T x + \varphi_{rs0}
\end{split}
\end{align}
Rozdělující plocha mezi třídami $ \omega_r $ a $ \omega_s $ je n-dimenzionální \textit{rovina}. Jedná se tedy o n-dimenzionální \textit{lineární diskriminační funkci}.

\item \textbf{Všechny třídy mají stejnou diagonální kov. matici $ C_r = C = \delta^2 I, r=1,...,R $}

Předpokladem je, že obrazy každé třídy mají statisticky nezávislé příznaky a každý příznak má stejnou varianci $ \delta^2 $. Geometricky to odpovídá situaci, kdy vzorky každé třídy vytváří shluky tvaru n-dimenzionálních koulí centrovaných kolem příslušné střední hodnoty. Potom $ det \, C = \delta^{2n} $ a $ C^{-1} = \frac{1}{\delta^2} I $. Předpokládejme, že všechny třídy jsou stejně pravděpodobné, tj. $ p(\omega_r) = p(\omega), \forall r=1,...,R $. Potom:
\begin{equation}
g_r(x) = - \frac{1}{2\delta^2} || x-\mu_r ||^2 + ln \, p(\omega) -\frac{n}{2} \, ln \, 2\pi - \frac{1}{2} \, ln(\delta^{2n}) = -k_1 \cdot || x-\mu_r ||^2 + k_2
\end{equation}
Konstanty $ k_1 > 0 $ a $ k_2 $ jsou shodné pro všechny třídy a výraz $ || x-\mu_r ||^2 $ představuje kvadrát Euklidovské vzdálenosti mezi vektorem $ x $ a střední hodnotou $ \omega_r $. Klasifikátor zařadí neznámý obraz $ x $ do té třídy $ \omega_r $, pro kterou je $ g_r(x) $ maximální. Z výrazu pro $ g_r(x) $ vyplývá, že  $ g_r(x) $ bude tím větší, čím bude $ || x-\mu_r ||^2 $ menší ($ k_1 > 0 $). Jedná se tedy v podstatě o \textit{klasifikátor podle minimální vzdálenosti}.
\end{enumerate}

\subsubsection*{Klasifikace podle minimální vzdálenosti}
Diskriminační funkce: $ g_r^*(x) = || x-\mu_r ||^2 $. Klasifikátor zařadí neznámý obraz $ x $ do té třídy, pro kterou bude $ g_r^*(x) $ minimální ($ \omega_r^* = \underset{r}{\mathrm{min}} \, d^2(x, \mu_r) $). Není to určitě nejlepší klasifikátor, ale je tu velká lákavost ho používat, protože stačí jediný obraz na třídu. Rozdělující nadrovina má tvar:
\begin{align}
\begin{split}
\varphi_{rs}(x) &= -k1 \cdot || x-\mu_r ||^2 + k_2 - \left[-k_1 \cdot || x-\mu_s ||^2 + k_2 \right] = \\
&= k_1 \cdot \left[x^Tx - 2\mu_sx + \mu_s^T\mu_s - x^Tx - 2\mu_rx + \mu_r^T\mu_r\right] \overset{!}{=} 0 \\
&=> (\mu_r - \mu_s)^T x - \frac{1}{2}(\mu_r^T\mu_r - \mu_s^T\mu_s) = 0
\end{split}
\end{align}
Rozdělující nadplochy mezi třídami jsou \textit{lineární}, jsou to n-dimenzionální roviny kolmé na úsečku $ \mu_r - \mu_s $, kterou půlí \footnote{Klasifikátor podle minimální vzdálenosti je ekvivalentní co do struktury lineárnímu klasifikátoru s $ R $ diskriminačními funkcemi, který může vytvořit až $ \frac{R (R-1)}{2} $ rozdělujících nadrovin. Má však obecně jiné parametry, tj. klasifikuje obecně jiným způsobem než lineární klasifikátor.}. Tento klasifikátor je velmi jednoduchý na implementaci - pro jeho nastavení stačí získat střední hodnoty každé třídy a pro neznámý obraz $ x $ ve fázi klasifikace vypočítat vzdálenost ke všem středním hodnotám (též nazýván \textit{klasifikátor se vzorovými etalony}. Pro svou jednoduchost je často nasazován i v případech, kdy není zabezpečena jeho optimální funkce podle kritéria minimální chyby (např. je málo početná trénovací množina nebo není znám typ rozložení nebo není známa disperzní matice ap.). To vede k negativním vlivům klasifikátoru:
\begin{itemize}
\item vzhledem k tomu, že využívá pouze střední hodnoty, nerespektuje tvar shluků jednotlivých tříd (pokud je odlišný od $ C_r = C = \delta^2 I $; tvar shluku koresponduje s tvarem disperzní matice).
\item nerespektuje případné odlišné apriorní pravděpodobnosti jednotlivých tříd
\end{itemize}
Dobrých výsledků dosáhneme, když budou třídy dobře distribuované (střední hodnoty dostatečně vzdálené, shluky dostatečně kompaktní a jednotlivé třídy stejně pravděpodobné).

\subsubsection*{Klasifikace podle nejbližšího souseda (Nearest Neighbour Classifier)}
Uvedené nevýhody klasifikátoru podle minimální vzdálenosti lze zmírnit často tím, že využijeme více vzorových etalonů pro každou třídu. Zvolíme-li pro každou třídu $ \omega_r $ $ S_r $ vzorových etalonů: $ \mu_{r1}, \mu_{r2}, \dots, \mu_{rS_r} $ , pak klasifikace probíhá podle pravidla vyjádřeného vztahem:
\begin{equation}
\omega_r^* = \underset{s,r}{\mathrm{argmin}} \, ||x-\mu{rs}|| = \underset{s,r}{\mathrm{argmin}} \, d(x, \mu{rs})
\end{equation}
Obraz $ x $ se tedy zařadí do té třídy $ \omega_r $, jejíž některý etalon má mezi všemi ostatními etalony nejmenší vzdálenost od $ x $. Tento způsob klasifikace má tu výhodu, že při dostatečně rozsáhlé trénovací množině se tvar rozdělujících funkcí pro jednotlivé třídy "blíží" Bayesovskému klasifikátoru. Na druhou stranu to však znamená značné zvýšení výpočetních nároků (klasifikátor si musí neustále pamatovat celou množinu vzorových etalonů - celou trénovací množinu) a při klasifikaci musíme počítat vzdálenost neznámého obrazu $ x $ ke všem vzorům.

\subsubsection*{Klasifikace podle k-nejbližších sousedů (k-Nearest Neighbour Classifier)}
Lepších výsledků lze často dosáhnout využitím tzv. rozhodovacího pravidla, kdy nejprve vyčíslíme všechny vzdálenosti $ ||x - \mu_{rs}|| \forall r=1,...,R \forall s=1,...,S_r $ a pak je pro každou třídu $ \omega_r $ uspořádáme tak, aby pro nový soubor $ ||x - \mu_{r[s]}|| $ platilo:
\begin{align*}
||x - \mu_{r[1]}|| \leq ||x - \mu_{r[2]}|| \leq \dots \leq ||x - \mu_{r[S_r]}||
\end{align*}
Klasifikátor pak zařadí obraz $ x $ do třídy $ \omega_r^* $ podle minima průměrné vzdálenosti k-nejbližších sousedů:
\begin{equation}
\omega_r^* = \underset{r}{\mathrm{argmin}} \, \frac{1}{k} \displaystyle{\sum_{k=1}^k} ||x - \mu_{r[i]}||, \qquad r=1,...,R
\end{equation}
Nevýhody:
\begin{itemize}
\item musím si pamatovat všechny obrazy
\item při každé klasifikaci náročné výpočty
\end{itemize}

\subsection{Klasifikátor s lineární diskriminační funkcí. Klasifikace do dvou a do více tříd.}
Pokud obrazy v jednotlivých třídách podléhají normálnímu rozložení a všechny třídy vykazují stejnou kovarianční matici $ C $, je optimální nastavení klasifikátoru podle kritéria minimální chyby (Bayesova kritéria) zabezpečeno \textit{lineárními diskriminačními funkcemi}. Vzhledem k jejich výhodným analytickým vlastnostem se jich ovšem využívá i v případech, kdy výše uvedené podmínky splněny nejsou a nebo, a to je častější případ, kdy ověření platnosti těchto podmínek je nepřiměřeně náročné (např. nelze statisticky prokázat typ rozložení vzhledem k malému počtu obrazů ap.). Zvolíme-li v takovém případě rozhodovací pravidlo založené na lineárních diskriminačních funkcích, musíme mít vždy na paměti, že jsme nezvolili optimální řešení s hlediska Bayesova kriteria minimální chyby. Přesto je třeba říci, že v případech, kdy obrazy jednotlivých tříd jsou dobře distribuované, tj. vytvářejí kompaktní shluky, které jsou od sebe dostatečně vzdálené (lineárně separabilní třídy) toto zjednodušení dostatečně vyhovuje.

Uvažme lineární diskriminační funkci $ g(x) = q_0 + q_1 x_1 + \dots q_n x_n = q_0 + \displaystyle{\sum_{i=1}^{n}} q_i x_i $, kde $ q_i $ jsou váhy funkce a $ q_0 $ je práh funkce.

Dále mějme $ ||q|| = \sqrt{q_1^2 + q_2^2 + \dots + q_n^2} $. Pro $ n=2 $:
\vspace{3cm}

\subsubsection*{Klasifikace do dvou tříd (dichotomie)}
Při klasifikaci do dvou tříd $ \omega_1 $ a $ \omega_2 $ stačí k rozhodnutí jediná diskriminační funkce:
\begin{equation}
g(x) = q_0 + \displaystyle{\sum_{i=1}^{n}} q_i x_i
\end{equation}
Pro $ g(x) > 0 $ je $ x \in \omega_1 $, pro $ g(x) < 0 $ je $ x \in \omega_2 $.

\subsubsection*{Klasifikace do více tříd}
\begin{enumerate}[label=(\alph*)]
\item Předpokládejme, že obrazy každé třídy jsou \textit{lineárně separovatelné} od obrazů všech ostatních tříd. Pak diskriminační funkce mezi třídami $ \omega_r $ a $ \bar{\omega_r} $ je:
\begin{equation}
g_r(x) = q_{r,0} + \displaystyle{\sum_{i=1}^{n}} q_{r,i} x_i
\end{equation}
a platí, že pro $ x \in \omega_r $ je $ g_r(x) > 0 $ a pro $ x \in \bar{\omega_r} $ je $ g_r(x) < 0 $. Klasifikátor pak rozhodne o zařazení $ x $ do té třídy $ \omega_r (r=1,...,R) $ pro níž je diskriminační funkce $ g_r(x) > 0 $. Problém je však v tom, že se může stát, že pro neznámé $ x $ bude hodnota více než jedné diskriminační funkce větší než $ 0 $. V takovém případě klasifikátor není schopen rozhodnout \footnote{Tento způsob klasifikace má jistá omezení, např. v prostoru dimenze $ n = 2 $ lze takto rozdělit maximálně $ R =3 $ dobře distribuované třídy.}.

Př. ($ n=2 $, $ R=3 $):
\vspace{4cm}

\item Předpokládejme, že obrazy každé třídy jsou \textit{po dvojicích lineárně separovatelné} od všech ostatních tříd. V tomto případě existuje celkově $ \frac{R(R-1)}{2} $ diskriminačních funkcí $ \varphi_{rs}(x); r,s = 1,...,R \land r \neq s $, které vytvářejí rozdělující roviny mezi obrazy všech dvojic tříd. Pro obraz $ x \in \omega_r $ pak platí $ \varphi_{rs} (x) > 0 \, \forall s \neq r $, viz\footnote{Samozřejmě platí $ \varphi_{rs}(x) = -\varphi_{sr}(x) $. V mnoha případech se nevyužívá všech $ \frac{R(R-1)}{2} $ diskriminačních funkcí. V tomto případě se opět objevují oblasti, pro které nejsme schopni rozhodnout a zařazení $ x $.}.

Př. ($ n=2 $, $ R=4 $):
\vspace{4cm}

\item Předpokládejme\footnote{Vylepšení ad a).}, že existují diskriminační funkce $ g_r(x), r=1,...,R $ z případu ad a). Vytvoříme rozdělující hyperplochy mezi třídami $ r $ a $ s $.
\begin{equation}
\varphi_{rs}(x) = g_r(x) - g_s(x) \overset{!}{=} 0
\end{equation}
Pro $ \varphi_{rs}(x) > 0 $ je $ g_r(x) > g_s(x) $. Z toho vyplývá, že klasifikátor zařadí $ x $ do $ \omega_r $, jestliže $ g_r(x) > g_s(x) \, \forall s=1,...,R; \, s \neq r $. Viz poznámky\footnote{Rozdělující funkce $ \varphi_{rs}(x) $ rozdělují obrazový prostor bezezbytku (nejsou hluché oblasti, kde není možno provést přiřazení).}.
\vspace{4cm}
\end{enumerate}

\textit{Hodnocení:} Je zřejmé, že případ ad a) není vhodný k aplikování vzhledem k vytváření rozsáhlých oblastí, ve kterých nejsme schopni provést jednoznačné přiřazení. Rozhodnutí mezi případem ad b) a ad c) závisí do značné míry na intuici (zvláště v prostorech vyšší dimenze). Obecně lze říci, že případ ad b) vyžaduje určení $ \frac{R(R-1)}{2} $ diskriminačních funkcí $ \varphi_{rs}(x) $, kdežto případ ad c) požaduje nalezení pouze $ R $ diskriminačních funkcí $ g_r(x) $. Jestliže se však počet tříd $ R $ blíží dimenzi $ n $ obrazového prostoru nebo se očekává, že obrazy jednotlivých tříd jsou špatně distribuované, bude možná postup podle ad b) lepším řešením.

\subsection{Metody nastavování klasifikátorů (trénování klasifikátorů).}
\begin{enumerate}[label=(\alph*)]
\item Známe-li celou trénovací množinu apriori, můžeme se pokusit o \textit{analytické řešení:}
\begin{itemize}
\item \textit{pravděpodobnostní diskriminační funkce}: je třeba určit a prokázat typ rozložení a hodnoty parametrů rozložení včetně apriorní pravděpodobnosti tříd
\item \textit{klasifikátor dle minima vzdálenosti}: je třeba určit střední hodnotu obrazů v každé třídě
\item \textit{klasifikátor podle nejbližšího či k-nejbližšího souseda}
\end{itemize}
Pro prostory vyšší dimenze nepoužitelné, navíc je třeba si pamatovat celou trénovací množinu. Dále není umožněno dotrénování klasifikátoru, pokud se objeví nové informace o trénovací množině. Častou chybou je nerespektování apriorních pravděpodobností tříd.
\item Trénovací množinu neznáme apriori: \textit{metody učení}
Učící se klasifikátor má dvě fáze:
\begin{itemize}
\item \textit{fáze učení}: postupně předkládány dvojice $ [x_k, \Omega_k] $ z trénovací množiny, nastavujeme parametry $ q $ tak, aby pro $ k \to \infty $ bude $ q \to q^* $ (optimální nastavení, minimální střední hodnota ztrát).
\item \textit{fáze klasifikace}: využívá se zkušenosti nashromážděné v parametrech $ q $ k predikci neznámých obrazů. Klasifikátor se chová jako jednoúčelový automat.
\end{itemize}
Střední ztráta:
\begin{equation}
J(q) = \displaystyle{\int_{X \times O}} Q(x, \Omega, q) \, dF(x, \Omega) = \displaystyle{\sum_{r=1}^R} \, p(\omega_r) \displaystyle{\int_{X}} Q(x, \Omega, q) p(x|\omega_r) \, dx
\end{equation}
Úkolem je najít takové $ q^* $, které minimalizuje $ J(q) $.
\begin{equation}
\underset{q}{\mathrm{grad}} \, J(q^*) = \displaystyle{\int_{X \times O}} \underset{q}{\mathrm{grad}} \, Q(x, \Omega, q^*) \, dF(x, \Omega) \overset{!}{=} 0
\end{equation}
Běžně ovšem neznáme distribuční (ani hustotní) funkce, proto se obracíme na rekurentní algoritmy, které obcházejí přímé řešení rovnice. Existují dva přístupy:
\begin{itemize}
\item metody učení založené na odhadování hustot ppsti
\item metody učení založené na přímé minimalizaci ztrát
\end{itemize}
\end{enumerate}

\subsubsection*{Metody učení založené na odhadování hustot ppsti}
Snaha stanovit distribuční funkci $ dF(x, \Omega) $ z trénovací množiny, poté se využije kritérium minimální chyby a dostáváme soustavu diskriminačních funkcí $ g_r(x) = p(x|\omega_r) \cdot p(\omega_r) $. Hledáme odhady $ \hat{p}(x|\omega_r) $ a $ \hat{p}(\omega_r) $. Žádané vlastnosti:
\begin{itemize}
\item \textit{nestrannost}: má zaručovat, že se odhad bude v průměru pohybovat kolem neznámé odhadované veličiny
\item \textit{konzistence}: s rostoucím počtem obrazů trénovací množiny se odhad blíží stále více k odhadované veličině
\item \textit{eficience}: eficientní odhad je odhad s nejmenší disperzí
\end{itemize}

Je-li trénovací množina vybrána nezávisle, lze odhad $ \hat{p}(\omega_r) $ apriorní ppsti určit podle:
\begin{equation}
\hat{p}(\omega_r) = \frac{K_r}{K}, \qquad r=1,...,R
\end{equation}
kde $ K_r $ je počet obrazů trénovací množiny, které patří do třídy $ \omega_r $ a $ K $ je celkový počet obrazů v trénovací množině. Podle velikosti apriorní informace o hledané hustotě rozdělujeme metody získávání odhadů hustotní funkce na \textit{parametrické} a \textit{neparametrické}.
\begin{enumerate}
\item \textit{Parametrické metody}: známe informaci o tvaru hustotní funkce $ p(x|\omega_r) $, ale neznáme parametry $ q $, který rozložení blíže popisuje (např. u Gausse: $ \mu_r $ a $ \delta_r $).
\begin{itemize}
\item \textit{Metoda momentů}

Teoretické momenty náhodných veličin se porovnávají s výběrovými momenty do takového stupně $ l $, kolik je neznámých parametrů.

výběrový průměr: $ \bar{x} = \frac{1}{K} \displaystyle{\sum_{k=1}^K x_k} $

výběrový rozptyl: $ S = \frac{1}{K-1} \displaystyle{\sum_{k=1}^K (x_k-\bar{x})(x_k-\bar{x})^T} $

$ l $-tý výběrový moment: $ M_l = \frac{1}{K} \displaystyle{\sum_{k=1}^K x_k^l} $
\item \textit{Metoda nejlepších nestranných lineárních odhadů}

Odhad parametrů $ q $ se uvažuje jako lineární funkce obrazů $ x_1, ..., x_K $: $ \hat{q} = c_1 x_1 + ... + c_K x_K $. Koeficienty $ c_k, k=1,...,K $ se určují z podmínek nestrannosti a minimální disperze odhadu $ \hat{q} $. Platí $ E \, \hat{q} = q $ a odhad $ \hat{q} $ parametru $ q $ je eficientní, jestliže minimalizuje stopu disperzní matice $ tr \, D \, \hat{q} $.

\item \textit{Metoda maximální věrohodnosti}

Metoda je založena na maximalizaci tzv. Fisherovy funkce věrohodnosti:
\begin{equation}
L(x_1, ..., x_K|q) = \displaystyle{\prod_{k=1}^K} p(x_k|q)
\end{equation}
Hledáme takový parametr $ q $, pro který je funkce maximální. Místo tohoto maxima lze hledat maximum logaritmu této věrohodnostní funkce $ \underset{q}{\mathrm{grad}} \, ln \, L(x_1,...,x_K|q) \overset{!}{=} 0 $. Např. pro normální rozdělení je nejlepším odhadem střední hodnoty aritmetický průměr $ \hat{\mu} = \frac{1}{N} \displaystyle{\sum_{i=1}^N} x_i $ a nejlepším odhadem kovarianční matice je: 
\begin{equation}
C = \frac{1}{N} \displaystyle{\sum_{i=1}^N} (x_i-\mu)(x_i-\mu)^T
\end{equation}
\end{itemize}
\item \textit{Neparametrické metody}: máme nulovou apriorní informaci, musíme odhadovat celý tvar hustotní funkce.
\begin{itemize}
\item \textit{Metoda histogramu}

Obrazový prostor rozdělíme na $ L $ disjunktních podmnožin $ A_l, l=1,...,L $ (obvykle to jsou n-rozměrné intervaly). Symbolem $ c_l $ označíme počet obrazů $ x_k $, které padnou do $ A_l $, dělený číslem $ K $. Za odhad $ \hat{p}(x|\omega_r) $ pak bereme po částech konstantní funkci, která na intervalu $ A_l $ nabývá $ c_l $. Nevýhodou je nutnost znalosti apriorního rozdělení obrazového prostoru na intervaly před fází učení. Odhad hustotní funkce:
\begin{equation}
\hat{p}(x|\omega_r) = \displaystyle{\sum_{l=1}^L c_l \cdot \varphi_l(x)}, \qquad \varphi_l = \begin{cases} 1 & x \in A_l \\ 0 & jinak \end{cases}
\end{equation}
\end{itemize}
\end{enumerate}

\subsubsection*{Metody učení založené na přímé minimalizaci ztrát}
Cílem je navrhnout parametry klasifikátoru, které budou minimalizovat ztrátu, rekurentním vypočítáváním odhadů $ \hat{q}(0) \to \hat{q}(1) \to \dots \to \hat{q}^* $. 
\begin{equation}
\underset{q}{\mathrm{grad}} \, J(q^*) = \displaystyle{\int_{X \times O}} \underset{q}{\mathrm{grad}} \, Q(x, \Omega, q^*) \, dF(x, \Omega) = 0
\end{equation}
Nabízí se využití gradientních metod. V praktických úlohách neznáme distribuční funkci a nelze tak vyčíslit $ \mathrm{grad} \, J(q) $. Navíc, při pevném $ q $ hodnota funkce $ Q(x, \Omega, q^*) $ náhodně kolísá v závislosti na $ x $ a $ \Omega $ - to je u gradientních metod nepoužitelné. Řešení rovnice hledáme pomocí metody \textit{stochastických aproximací}. Základní algoritmus přímé minimalizace ztrát:
\begin{equation}
q(k+1) = q(k) - C_{k+1} \, \underset{q}{\mathrm{grad}} \, Q[x(k+1), \Omega(k+1), q(k)] \qquad k=1,...,K
\end{equation}
$ C_k $ je čtvercová matice (obvykle $ C_k = c_k \cdot I $), $ [x(k), \Omega(k)] $ je k-tá dvojice trénovací množiny a $ q $ je vektor parametrů. Vhodnou volbou $ Q(x, \Omega, q) $ a $ C_k $ lze získat téměř všechny algoritmy přímé minimalizace ztrát.

Položíme $ x_0 = 1 $: $ x^T = [x_0, x_1, ..., x_n] $ a váhový vektor rozšíříme o práh $ q_0 $: $ q^T = [q_0, q_1, ..., q_n] $. Diskriminační funkce má tvar $ g(x) = q^T \cdot x $ a rozhodovací pravidlo $ \omega = \mathrm{sign} \, g(x) $. Po zavedení pásma necitlivosti $ \delta $ volíme:
\begin{equation}
\underset{q}{\mathrm{grad}} \, Q(x, \Omega, q) = \begin{cases} 0 & q^T x \Omega \geq \delta \\ -x \Omega & q^T x \Omega < \delta \end{cases}
\end{equation}
Dosadíme-li do vztahu pro rekurentní výpočet $ q $, dostaneme algoritmus učení:
\begin{align}
\begin{split}
q(k+1) &= q(k) - C_{k+1} \, \underset{q}{\mathrm{grad}} \, Q[x(k+1), \Omega(k+1), q(k)] = \\
&= \begin{cases} q(k) & q^T(k) x(k+1) \Omega(k+1) \geq 0 \\ q(k) + C_{k+1} x(k+1) \Omega(k+1) & q^T(k) x(k+1) \Omega(k+1) < 0 \end{cases}
\end{split}
\end{align}
\begin{enumerate}[label=(\alph*)]
\item \textit{Rosenblattův algoritmus}

$ C_k = C_{k+1} = 1 $, $ \delta = 0 $:
\begin{align}
\begin{split}
q(k+1) = \begin{cases} q(k) & q^T(k) x(k+1) \Omega(k+1) \geq 0 \\ q(k) + x(k+1) \Omega(k+1) & q^T(k) x(k+1) \Omega(k+1) < 0 \end{cases}
\end{split}
\end{align}
Nedostatkem je, že nikdy nezjistíme absolutně přesný klasifikátor. 
\item \textit{Metoda konstantních přírůstků}

$ C_k = \frac{\beta}{||x(k)||^2}, \, \beta > 0 $
\begin{align}
\begin{split}
q(k+1) = \begin{cases} q(k) & q^T(k) x(k+1) \Omega(k+1) \geq \delta \\ q(k) + \frac{\beta}{||x(k+1)||^2} x(k+1) \Omega(k+1) & q^T(k) x(k+1) \Omega(k+1) < \delta \end{cases}
\end{split}
\end{align}
\item \textit{Upravená metoda konstantních přírůstků}

Připočítáváme vektor $ \frac{\beta}{||x(k+1)||^2} x(k+1) \Omega(k+1) $ tolikrát, až pro určitý obraz $ x(k+1) $ dosáhneme správné klasifikace, tj. bude platit $ q^T(k+1) x(k+1) \Omega(k+1) \geq \delta $.
Může se stát, že nikdy nedojdeme k řešení.
\item \textit{Relaxační metoda učení}

\begin{align}
\begin{split}
q(k+1) = \begin{cases} q(k) & q^T(k) x(k+1) \Omega(k+1) \geq \delta \\ q(k) + 2 C_{k+1} x(k+1) \Omega(k+1) [\delta - q^T(k) x(k+1) \Omega(k+1)] & q^T(k) x(k+1) \Omega(k+1) < \delta \end{cases}
\end{split}
\end{align}

kde $ c_k = \frac{\sigma}{2 ||x(k)||^2} $ a $ \sigma \in (0, 2) $.
\end{enumerate}

\subsection{Metody shlukové analýzy (učení bez učitele).}
Někdy se stane, že je k dispozici jen trénovací množina bez údajů o správné klasifikaci, někdy chybí i informace o počtu tříd. Úkolem je nalézt shluky obrazů, tj. takové skupiny jejichž prvky jsou si vzájemně podobné (geometricky blízké). Lze aplikovat pouze na data, kde ty shluky opravdu jsou.

Požadavky pro míry podobnosti: $ d(x_i, x_i) = 0 $; $ d(x_i, x_j) \geq 0, \, i \neq j $; $ d(x_i, x_j) = d(x_j, x_i) $.

Míry podobnosti mezi dvěma obrazy $ x $ a $ x' $:
\begin{itemize}
\item $ d(x, x') = || x-x' || $
\item Euklidova vzdálenost: $ d(x, x') = \sqrt{\displaystyle{\sum_{i=1}^n}(x_i - x_i')^2} $
\end{itemize}

Míry podobnosti mezi dvěma shluky $ T_i $ a $ T_j $:
\begin{itemize}
\item minimální míra: $ D_{min}(T_i, T_j) = \underset{x \in T_i; x' \in T_j}{\mathrm{min}} d(x, x') $
\item maximální míra: $ D_{max}(T_i, T_j) = \underset{x \in T_i; x' \in T_j}{\mathrm{max}} d(x, x') $
\item průměrná míra: $ D_{mean}(T_i, T_j) = \frac{1}{s_i \cdot s_j} \displaystyle{\sum_{x \in T_i} \sum_{x' \in T_j}} d(x, x') $
\item centroidní míra: $ D_{c}(T_i, T_j) = || \frac{1}{s_i} \displaystyle{\sum_{x \in T_i} x} - \frac{1}{s_j} \displaystyle{\sum_{x' \in T_j} x'} || = d(\bar{x}, \bar{x}')$
\end{itemize}

\subsubsection*{Nehierarchické shlukovací metody}
Snaha provést optimální rozklad dané množiny $ T $ do předem známého počtu $ R $ shluků. Hledáme takové rozdělení, pro které nabývá kritérium $ J $ extrém.
\begin{equation}
J = \displaystyle{\sum_{i=1}^R J_i} = \displaystyle{\sum_{i=1}^R \sum_{x \in T_i} d^2(x, \mu_i)}
\end{equation}
\begin{enumerate}
\item \textbf{McQueenova metoda k-means (1967)}
Algoritmus se snaží minimalizovat celkový ukazatel jakosti $ J $ rozdělením obrazů do shluků.

Dáno:
\begin{description}
\item[$ T = \{x_i\}_{i=1}^N $] ... obrazy trénovací množiny
\item[$ R $] ... počet tříd
\item[$ T_i(k) $] ... množina obrazů i-tého shluku v k-tém kroku algoritmu
\item[$ \mu_i(k) $] ... střední (průměrná) hodnota i-tého shluku v k-tém kroku
\item[$ J_i(k) $] ... hodnota kritéria i-tého shluku v k-tém kroku
\item[$ s_i(k) $] ... počet obrazů ve shluku $ T_i $ v k-tém kroku
\end{description}
Postup:
\begin{enumerate}[label=(\roman*)]
\item Zvol $ R $ počátečních středů shluků $ \mu_1(1), ..., \mu_R(1) $.
\item V k-tém iteračním kroku se rozdělují obrazy trénovací množiny do $ R $ shluků $ T_1(k), ..., T_R(k) $ podle vztahu $ x \in T_j(k) $ jestliže $ d(x, \mu_j(k)) < d(x, \mu_i(k)) \, \forall i,j=1,...R; \, i \neq j $. Vztah je postupně aplikován pro všechny obrazy z množiny $ T $.
\item Z výsledků předchozího kroku vypočti pro každý shluk nový střed, tj. $ \mu_j(k+1), \, j=1,...,R $ klasickým zprůměrováním (zaručí, že $ J_j(k+1 $ bude minimální):
\begin{equation}
\mu_j(k+1) =  \frac{1}{s_j(k)} \displaystyle{\sum_{x \in T_j(k)} x}, \qquad j=1,...,R
\end{equation}
\item Jestliže $ \mu_j(k+1) = \mu_j(k) \, j=1,..,R $, algoritmus dokonvergoval a procedura je ukončena. Jinak jdi na krok (ii). Alternativně lze proces ukončit v případě, že pokles hodnoty kriteriální funkce je již nevýznamný.
\end{enumerate}

Funkce metody je ovlivněna zejména specifickým počtem shluků a volbou počátečních středů shluků. Metoda sice pro vhodná data poskytuje přijatelné výsledky, ale dosažení globálního minima ukazatele jakosti  procesu shlukování není zaručeno (konvergence nejčastěji končí v nějakém lokálním minimu).

\item \textbf{Iterativní optimalizace}
V k-tém iteračním kroku změníme zařazení jednoho obrazu.

Postup:
\begin{enumerate}[label=(\roman*)]
\item Proveď  počáteční rozklad $ N $ obrazů do $ R $ shluků a vypočti hodnotu kriteriální funkce $ J $ a urči $ \mu_i(1), \, i=1,...,R $.
\item Vyber obraz $ \hat{x} $, kde např. $ \hat{x} \in T_i $ (nejlépe nějaký systematický výběr).
\item Jestliže $ s_i(k) = 1 $, jdi na bod (vi)
\item Vypočti:
\begin{align}
\begin{split}
A_i &= \frac{s_i(k)}{s_i(k) - 1} d^2(\hat{x}, \mu_i(k)) \\
A_j &= \frac{s_j(k)}{s_j(k) + 1} d^2(\hat{x}, \mu_j(k)), \qquad \forall j \neq i
\end{split}
\end{align}
Urči $ j^* = \underset{j}{\mathrm{argmin}} \, A_j $. Jestliže $ A_i > A_{j^*} $, přesuň $ \hat{x} $ do $ T_{j^*} $. Jinak ponech $ \hat{x} $ v $ T_i $.
\item Aktualizuj $ J $, $ \mu_i $ a $ \mu_{j^*} $.
\item Jestliže se $ J $ po pokusu přesunout postupně všech $ N $ obrazů trénovací množiny nezměnila, proces ukonči. Jinak přejdi do bodu (ii).
\end{enumerate}

Pokud bychom chtěli dosáhnout opravdu globálního minima $ J $, mohli bychom vyzkoušet teoreticky všechny možnosti rozkladu, ale existuje obecně $ \frac{R^N}{R!} $ možností (pro $ R = 3 $ a $ N = 100 $ je to $ \approx 10^{47} $ možností).
\end{enumerate}

\subsubsection*{Hierarchické shlukovací metody}
Většinou aplikujeme, pokud není známa informace o počtu tříd.
\begin{enumerate}[label=(\Alph*)]
\item \textit{Aglomerativní přístup}
Vycházíme z jednotlivých obrazů, které postupně shlukujeme, až dojde ke spojení všech obrazů do jedné množiny.
\begin{enumerate}[label=\arabic*.]
\item \textbf{Metoda shlukové hladiny}
Celé množině obrazů $ T = \{x_1, ..., x_N\} $ postupně přiřazujeme posloupnost rozkladů $ B_0 $ až $ B_{N-1} $ a každému vzniklému shluku $ A $ přiřazujeme tzv. shlukovací hladinu $ h(A) \geq 0 $.

Postup:
\begin{enumerate}[label=(\roman*)]
\item Rozklad $ B_0 $ tvoří jednotlivé obrazy, tj. $ A_{0j} = \{x_j\} $ a $ h(A_{0j}) = 0, \, j=1,...,N $.
\item V i-tém kroku pro $ 1 \leq i \leq N-1 $ vybereme jedinou dvojici shluků (obrazů) $ A_{i-1, u} $ a $ A_{i-1, v} $, pro kterou nastala nejmenší vzdálenost (ve smyslu zvolené metriky).
\begin{itemize}
\item provedeme sjednocení shluků $ A_{i-1, u} \cup A_{i-1, v} < A_{i, l}, \, l=1,...,N-1 $;
\item utvoříme rozklad $ B_i = \{A_{i,1},...,A_{i,N-1}\} $, kde všechny shluky s výjimkou sjednoceného $ A_{i,l} $ přechází do $ B_i $ beze změny;
\item položíme $ A_{i,l} = \underset{u,v}{\mathrm{min}} D(A_{i-1, u}, A_{i-1, v}) $
\end{itemize}
\item Bod (ii) opakujeme, až v posledním kroku procedury vznikne jediný shluk.
\end{enumerate}
\end{enumerate}
\item \textit{Divisní přístup}
Celkový shluk obrazů určený ke klasifikaci postupně rozdělujeme a získáme hierarchický systém podmnožin.
\begin{enumerate}[label=\arabic*.]
\item \textbf{Jednoprůchodový heuristický algoritmus hledání shluků}
Je zadána trénovací množina $ T = \{x_1, ..., x_N\} $. Definujeme ve vztahu se zvolenou mírou podobnosti neznámý práh $ t > 0 $.
\begin{enumerate}[label=(\roman*)]
\item Z obrazů trénovací množiny vybereme jeden, např. $ x_{(1)} \in T $ a ztotožníme ho se středem prvního shluku $ \mu_1 = x_{(1)} $.
\item Z trénovací množiny vybereme obraz $ x_{(2)} \in T $ a vypočteme vzdálenost $ d_{21} = d(x_{(2)}, \mu_1) $. Jestliže $ d_{21} > t $, pak zavedeme nový shluk se středem $ \mu_2 \coloneqq x_{(2)} $. Jinak přidělíme obraz $ x_{(2)} $ do shluku se středem $ \mu_1 $.
\item Předpokládáme, že shluk se středem $ \mu_2 $ byl zaveden, poté vybereme obraz $ x_{(3)} \in T $ a vyčíslíme $ d_{31} = d(x_{(3)}, \mu_1) $ a $ d_{32} = d(x_{(3)}, \mu_2) $. Jestliže $ d_{31} > t $ a $ d_{32} > t $, zavedeme nový shluk se středem $ \mu_3 \coloneqq x_{(3)} $. Jinak přidělíme obraz $ x_{(3)} $ do shluku, k jehož středu má nejmenší vzdálenost.
\item Postupujeme analogicky, až rozdělíme všechny obrazy trénovací množiny.
\end{enumerate}
Výsledky závisí na prvním vybraném středu shluku; pořadí, v němž jsou obrazy uvažovány; hodnotě $ t $; geometrických vlastnostech dat. Algoritmus je velmi jednoduchý a rychlý a stanovuje první náhled na trénovací množinu obrazů. Podle velikosti prahu $ t $ se mění i výsledný počet shluků. Metoda vyžaduje pouze jediný průchod trénovací množinou, ale v praxi je potřeba rozsáhlé experimentování s hodnotou prahu a startovacím obrazem.

\item \textbf{Metoda řetězové mapy}
Základním krokem při vytváření řetězové mapy je přeuspořádání dat. Z trénovací množiny vybereme libovolný "startovací" obraz $ x_{(1)} $, najdeme jeho nejbližšího souseda $ x_{(1)[1]} $ a položíme $ x_{(2)} = x_{(1)[1]} $ (druhá položka seznamu). Další položkou bude nejbližší soused k $ x_{(2)} $, atd. Nejbližšího souseda vždy vybíráme z množiny dosud neuspořádaných obrazů. Proces pokračuje, dokud nezískáme posloupnost ze všech obrazů trénovací množiny.
\begin{equation}
\tilde{T} = \{x_{(1)}, ..., x_{(N)}\}
\end{equation}
S i-tým členem této posloupnosti potom spojíme vzdálenost $ d_i = d(x_{(i), x_{i+1}}, \, i=1,...,N-1 $. Řetězovou mapu získáme jako závislost $ d_i = f(i) $. Znázorníme-li tuto závislost graficky, pak pokud jsou obrazy trénovací množiny dobře distribuovány, tj. vytvářejí v prostoru kompaktní shluky, které jsou od sebe dostatečně vzdálené, lze typicky na diagramu nalézt souvislé oblasti relativně malých hodnot $ d_i $, které jsou od sebe odděleny (relativně) vyšší hodnotou $ d_j $. Tyto zvýšené hodnoty $ d_j $ odpovídají hranicím shluků.

\item \textbf{Metoda MAXIMIN (maximum-minimum)}
Jedná se o jednoduchý heuristický algoritmus, který je vhodný pro odhad počtu shluků v obrazovém prostoru. Je dána množina obrazů $ T = \{x_k\}_{k=1}^N $ a volitelná konstanta $ q > 0 $.

Postup:
\begin{enumerate}[label=(\roman*)]
\item Z trénovací množiny vybereme "startovací" obraz a ztotožníme ho se středem prvního shluku, tj. $ \mu_1 \coloneqq x_{(1)} $.
\item Dále zvolíme (ve smyslu zvolené metriky) nejvzdálenější obraz k $ \mu_1 $ a ztotožníme ho se středem druhého shluku, tj. $ \mu_2 \coloneqq x_{(2)} $.
\item Pro každý obraz z trénovací množiny (bez $ x_{(1)} $ a $ x_{(2)} $) vyčíslíme vzdálenosti k $ \mu_1 $ a k $ \mu_2 $ a vždy vybereme tu minimální.
\item Z uchovaných minimálních vzdáleností vybereme tu maximální a porovnáme její velikost s velikostí vzdálenosti $ d(\mu_1, \mu_2) $ a obraz, pro který tato maximální vzdálenost nastala, označíme jako $ x_{(3)} $.
\item Jestliže je ta maximální vzdálenost větší než $ q \cdot d(\mu_1, \mu_2) $, zavedeme nový shluk se středem $ \mu_3 \coloneqq x_{(3)} $, jinak shlukovací proces končí.
\item V dalším kroku algoritmu postupujeme stejně jako v kroku (iii), ale pro tři středy shluků. To znamená, že z minimálních vzdáleností obrazů trénovací množiny ($ T \setminus \{x_{(1)}, x_{(2)}, x_{(3)} \} $) ke středům shluků $ \mu_1 $, $ \mu_2 $ a $ \mu_3 $ vybereme maximální a opět posoudíme, je-li větší než q-násobek např. průměru vzdáleností mezi již vytvořenými středy shluků. Postup opakujeme pro vzrůstající počet (středů) shluků, dokud se nová maximální vzdálenost nedostane do sporu s podmínkou vytvoření nového středu shluku.
\end{enumerate}
Po nalezení počtu shluků a jejich středů lze provést rozdělení obrazů trénovací množiny do jednotlivých shluků podle kritéria minimální vzdálenosti, přičemž za vzorové obrazy považujeme středy shluků.

\item \textbf{Metody binárního dělení}
Předpokládejme, že jsme v procesu shlukování získali $ R $ shluků. V dalším postupu obvykle provádíme zařazování neznámých obrazů do nejvhodnějších shluků podle pravidla nejbližšího souseda $ \to $ musíme nalézt takový shluk, jehož centroid má k neznámému obrazu $ x_i $ nejmenší vzdálenost. Při vyčerpávajícím porovnání se všemi centroidy narážíme při velkém $ R $ na velké množství operací. Metoda binárního dělení umožňuje redukci množství operací.
\begin{itemize}
\item \textbf{Rovnoměrné binární dělení}
Požadovaný počet výsledných shluků $ R $ musí být mocninou $ 2 $. V prvním kroku dělení je prvotní shluk obrazů trénovací množiny rozdělen na dva subshluky a každý z těchto subshluků je poté v dalším kroku rozdělen na další dva subshluky, atd. Proces končí, až je dosažen požadovaný konečný počet shluků $ R $. Platí $ R = 2^B $, kde $ B $ je počet kroků dělení. Proces lze znázornit prohledávacím stromem.

Při rovnoměrném dělení se mechanicky rozdělují všechny koncové shluky (reprezentované v prohledávacím stromu aktuálními koncovými uzly). Při tomto procesu se může stát, že v některém subshluku zůstane několik nebo dokonce jen jediný obraz $ \to $ další dělení takových shluků výrazně nepřispívá k minimalizaci celkového zkreslení.

\item \textbf{Nerovnoměrné binární dělení}
Zde výsledný počet shluků $ R $ nemusí být mocninou $ 2 $. Abychom zajistili nižší celkové zkreslení při daném počtu shluků, je vhodné nedělit prohledávací strom rovnoměrně, ale v každém kroku subdělícího procesu vyčíslit zkreslení všech koncových shluků a rozdělit ten shluk, který přispívá do celkového zkreslení největší měrou. Po každém kroku algoritmu, který měl za následek rozdělení nějakého subshluku zjistíme, zda již nebylo dosaženo stanoveného počtu shluků nebo zda velkost celkového zkreslení nepoklesla pod stanovenou mez. Pokud některá z těchto podmínek byla splněna, proces dělení končí.

\end{itemize}
\end{enumerate}
\end{enumerate}


\subsection{Výběr informativních příznaků.}
Velikost vektorů příznaků všech vzorků uvažovaných pro stejnou úlohu musí být shodná. Pokud není, lze přemýšlet o úpravě těchto vektorů, která obecně velmi závisí na fyzikální podstatě jednotlivých jevů (příznaků). Pokud to fyzikální podstata dovoluje, můžeme řetězce lineárně natáhnout, popř. lineárně smrštit, na požadovanou velikost. Plyne-li z fyzikální podstaty vektorů příznaků možnost lineárního kolísání (nejčastěji v časové ose), lze pro výpočet vzdálenosti takových vektorů využití metody tzv. nelineárního borcení jednoho z pozorovaných obrazů (\textit{DTW - dynamic time warping}).

Obecně při výběru informativních příznaků je třeba zachovat minimální hodnoty disperzí uvnitř jednotlivých tříd a maximální hodnoty disperzí mezi třídami.

\subsubsection*{Extrakce}
Metoda extrakce výběru minimálního počtu informativních příznaků je založena na nejlepší aproximaci původních obrazů z prostoru $ H_m $ o dimenzi $ m $ obrazy z prostoru $ H_n $ o dimenzi $ n < m $, a to ve smyslu minima střední kvadratické odchylky (Karhunen-Loevův rozvoj).
\vspace{2cm}

\subsubsection*{Selekce}
Definujeme míru diskriminativnosti množiny příznaků $ J = \mathrm{tr}(\hat{T}_{AKVI}^{-1} \cdot \hat{T}_{AKT}) $. Ta se rovná největšímu vlastnímu číslu matice $ T_{AKVI}^{-1} \cdot T_{AKT} $. Výběr příznaků založený na míře diskriminativnosti se provádí iterativně:
\begin{enumerate}[label=(\roman*)]
\item Vypočte se míra diskriminativnosti pro celý soubor příznaků.
\item Vypočte se míra diskriminativnosti při vynechání jednoho příznaku. Tento krok se provede n-krát (vynechávají se postupně příznaky $ v_1, ..., v_n $)
\item Z původního souboru příznaků vyloučíme definitivně ten příznak, při jehož vynechání v kroku (ii) došlo k nejmenšímu poklesu míry diskriminativnosti. Takový příznak je zřejmě nejméně informativní.
\end{enumerate}
\vspace{2cm}

\section{Neuronové sítě [NEU]}

\begin{table}[H]
\centering
\begin{tabular}{p{4cm} p{12cm}}
\textit{vyučující:}             & Doc. Dr. Ing. Vlasta Radová \\
\textit{ročník/semestr studia:} & 5.ročník/ZS \\
\textit{datum zkoušky:}         & 5. 1. 2017 \\
\textit{hodnocení:}             & 1 \\
\textit{cíl předmětu (STAG):}   & \\
\multicolumn{2}{p{16cm}}{Cílem předmětu je seznámit studenty se základními typy umělých neuronových sítí a s možnostmi jejich využití.}
\end{tabular}
\end{table}

\subsection{Základní umělé modely neuronu, vlastnosti, souvislost s biologickým neuronem.}
Lidský mozek se skládá ze 100 miliard neuronů, které mezi sebou komunikují prostřednictvím sítě vazeb. Podnět z receptorů (zrak, sluch, čich, chuť, hmat) je ve formě elektrických impulsů přenášen do centrálního nervového systému, kde je zpracováván.

\subsubsection*{Biologický neuron}
Základní buňka biologických neuronových sítí.
\begin{itemize}
\item \textit{soma} (tělo)
\item \textit{axon} (výstup): jediný, dlouhý až 60 cm
\item \textit{dendrity} (vstupy): krátké do 3 mm, je jich až několik tisíc na neuron
\item \textit{synapse} (rozhraní): jednosměrné brány umožňující přenos signálu pouze ve směru axon $ \to $ dendrita, je jich asi 10000 na neuron
\end{itemize}
Přenášené signály jsou elektrické impulsy, jejich přenos je ovlivněn uvolňováním budicích (excitátory) a tlumicích (inhibitory) látek v synapsích. Překročí-li hodnota budicích signálů hodnotu tlumicích signálů o určitý \textit{práh}, nastává tzv. \textit{aktivace neuronu} - na jeho výstupu se objeví impuls. Po vygenerování impulsu se neuron na určitou dobu (tzv. období pauzy) dostane do stavu, kdy nereaguje na žádné podněty $ \to $ chování neuronu lze popsat diskrétně v čase. Období pauzy není pro všechny neurony stejně dlouhé $ \to $ neurony v mozku pracují asynchronně.

\subsubsection*{McCullochův-Pittsův model (1943)}
Vstupy $ x_1,...,x_n $ mohou nabývat pouze hodnot 0 nebo 1 podle toho, zda je přítomen signál nebo ne. Váhy $ w_1,...,w_n $ nabývají hodnot 1 pro budící signály a -1 pro tlumící signály. Dále je definován práh $ b $. Jedná se tedy o jednoduchý binární model. Váhy jsou pevně nastaveny a práh je rovněž pevně stanoven.

\subsubsection*{Perceptron (1958)}
Diskrétní verze perceptronu byla poprvé prezentována Frankem Rosenblattem. Pro výstup platí:
\begin{equation}
y(k+1) = f\left[ \displaystyle{\sum_{i=1}^n} w_i x_i(k) + b \right] = f(w^T \cdot x(k) + b)
\end{equation}
\begin{description}
\item[$ w $] ... váhový vektor
\item[$ x $] ... vstupní vektor
\item[$ z = w^T \cdot x(k) + b $] ... aktivační hodnota
\item[$ f(\cdot) $] ... aktivační funkce, nejčastěji používané:
\begin{itemize}
\item \textit{bipolární binární}: $ f(z) = \mathrm{sgn}(z) = \begin{cases} 1 & z \geq 0 \\ -1 & z < 0 \end{cases} $
\item \textit{unipolární binární}: $ f(z) = \begin{cases} 1 & z \geq 0 \\ 0 & z < 0 \end{cases} $ 
\item \textit{bipolární spojitá}: $ f(z) = \frac{2}{1 + e^{-\lambda \cdot z}}-1 $
\item \textit{unipolární spojitá}: $ f(z) = \frac{1}{1 + e^{-\lambda \cdot z}} $
\item \textit{lineární}: $ f(z) = \lambda \cdot z $
\end{itemize}
\end{description}

\subsection{Základní typy neuronových sítí. Způsoby činnosti a učení neuronových sítí.}
Umělá neuronová síť vznikne spojením jednotlivých modelů neuronů. Výsledná funkce sítě je určena způsobem propojení jednotlivých neuronů (tzv. topologií sítě), váhami těchto spojení a způsobem činnosti jednotlivých neuronů (aktivačními funkcemi). Mezi základní typy patří:
\begin{itemize}
\item \textit{vícevrstvé dopředné neuronové sítě (feedforward nets)}: výstup jedné vrstvy je připojen na vstup následující vrstvy a signál se šíří pouze ze vstupu sítě na její výstup
\item \textit{neuronové sítě se zpětnou vazbou (recurrent, feedback nets)}: oproti dopředným sítím se zde signál šíří také z výstupu sítě zpět na její vstup
\end{itemize}

\subsubsection*{Fáze činnosti}
\begin{enumerate}
\item \textit{Fáze nastavování}: cílem je nastavit váhy a prahy sítě tak, aby prováděla požadovanou činnost (předpokládá se, že je dána topologie sítě); Provádí se buď výpočtem (lze jen u naprosto jednoduchých sítí) a nebo učením - trénováním
\item \textit{Fáze pracovní}: síť reaguje na předložené vstupy podle svého předchozího nastavení
\end{enumerate}

\subsubsection*{Způsoby učení}
\begin{itemize}
\item \textit{učení s učitelem (supervised learning)}: předpokládáme, že máme k dispozici tzv. trénovací množinu, tj. množinu dvojic $ \mathrm{[input, desired\_output]} $, které jsou síti postupně předkládány. Pro každý předložený vstup neuronová síť vygeneruje skutečný výstup, který se porovná s požadovaným výstupem, a trénovací algoritmus upraví nastavení vah a prahů tak, aby rozdíl mezi skutečným a požadovaným výstupem byl minimální. Každá dvojice se síti předkládá opakovaně - trénování probíhá v trénovacích cyklech (epochách), kdy během jedné epochy jsou sítě předloženy všechny dostupné dvojice. Pořadí výběru dvojic má vliv na výsledek učení. Dobře natrénovaná síť je schopna to, co se naučila, zobecňovat (schopnost generalizace). Trénování může být \textit{dávkové} (váhy a prahy se mění po předložení více dvojic) nebo \textit{sekvenční} (váhy a prahy se mění po každé dvojici).
\item \textit{učení bez učitele (unsupervised learning)}: síti jsou předkládány pouze vstupy, informace učitele (požadovaný výstup) chybí. Síť se sama snaží najít zákonitosti ve vstupních datech a nastavit své váhy a prahy tak, aby na podobné vstupy reagovala podobnými výstupy. Podobnost je většinou definována eukleidovskou vzdáleností a dochází tak ke shlukování vstupních dat.
\end{itemize}

\subsection{Algoritmus backpropagation.}

\subsection{Sítě se zpětnou vazbou. Hopfieldova neuronová síť.}
S předpokladem čtvercové matice $ W $ se výstup sítě počítá:
\begin{equation}
y_j(k+1) = f\left[\displaystyle{\sum_{i=1}^J} \, w_{ji} y_j(k) + b_j\right]
\end{equation} 
Přenos může být:
\begin{itemize}
\item \textit{synchronní}: všechny výstupy přepočítám naráz, tedy pro vstupy v kroku $ (k+1) $ používám výstupy z kroku $ (k) $
\item \textit{asynchronní}: přepočítávám jeden výstup po druhém, tedy např. pro vstup $ y_2(k+1) $ používám výstup $ y_1(k+1) $, tj. v každém kroku se vypočítává výstup pouze jednoho neuronu
\end{itemize}
Proces samovolného přechodu končí buď v \textit{rovnovážných stavech}, kdy $ y(k+1) = y(k) $, nebo v \textit{rovnovážných cyklech} tvořených stavy, mezi kterými síť kmitá. Jestliže rekurentní síť pracuje v asynchronním režimu, váhová matice je symetrická a prvky na diagonále jsou nezáporné $ \to $ síť vždy konverguje do rovnovážného stavu. Jestliže rekurentní síť pracuje v synchronním režimu a váhová matice je symetrická, pak síť vždy konverguje do rovnovážného stavu nebo cyklu délky 2.

\subsubsection*{Hopfieldova síť}
Jedná se o jednovrstvou rekurentní síť s neurony s bipolární binární aktivační funkcí, symetrickou váhovou maticí s nulami na diagonále a nulovým prahovým vektorem ($ w_{ij} = w_{ji} \, \forall i,j, i \neq j; \, \land \, w_{ii} = 0 $). Pro výstup i-tého neuronu platí:
\begin{equation}
y_i(k+1) = \mathrm{sgn}\left[ \displaystyle{\sum_{i=1}^n w_{ij} y_j(k)} \right]
\end{equation}
\begin{description}
\item[$ y(k) $] ... výstup sítě v čase $ (k) $ $ = $ stav sítě v čase $ (k) $
\item[$ y(0) $] ... inicializační stav sítě
\end{description}
Po inicializace v čase $ k=0 $ přechází síť samovolně z jednoho stavu do druhého (jedná se o dynamický systém). Ke změně stavu sítě může docházet synchronně či asynchronně. Při asynchronním přenosu se neuron, jehož výstup se bude přepočítávat, obvykle vybírá náhodně $ \to $ \textit{stochastická asynchronní rekurze}. 

U Hopfieldovy sítě nedochází k učení, váhy sítě jsou určovány pomocí tzv. \textit{záznamového algoritmu}, během kterého se do sítě zaznamenávají požadované rovnovážně stavy. Pro váhovou matici platí:
\begin{equation}
W = \left[ \displaystyle{\sum_{p=1}^P} u_p \cdot u_p^T \right] - P \cdot I
\end{equation}
\begin{description}
\item[$ u_p $] ... tzv. prototypy, tj. rovnovážné stavy, které mají být do sítě zaznamenány (dimenze $ n $)
\item[$ P $] ... počet zaznamenaných prototypů
\item[$ I $] ... identická matice řádu $ n $
\end{description}
Pro Hopfieldovu síť lze definovat tzv. \textit{výpočetní energii} ve tvaru:
\begin{equation}
E(y) = -\frac{1}{2} y^T \cdot W \cdot y
\end{equation}
Lze ukázat, že při přechodu sítě z jednoho stavu do jiného se tato energie nezvyšuje a v rovnovážném stavu (resp. cyklu) je minimální. Pro každý rovnovážný stav $ u $ existuje tzv. \textit{komplementární stav} $ u' $, pro který platí $ u' = -u $. Tento stav je rovněž rovnovážným stavem, i když v průběhu záznamového algoritmu nebyl do sítě zaznamenán. Zda proces přechodu sítě z jednoho stavu do jiného skončí v požadovaném nebo komplementárním stavu závisí při asynchronním režimu na pořadí přepočítávání výstupů jednotlivých neuronů $ \to $ nelze ovlivnit. Do Hopfieldovy sítě lze spolehlivě zaznamenat maximálně $ P \leq 0.14 \cdot n $ rovnovážných stavů ($ n $ je počet neuronů sítě). Pokud je v síti zaznamenáno rovnovážných stavů více, může proces synchronní i asynchronní rekurze skončit v tzv. \textit{falešném rovnovážném stavu}, který neodpovídá žádnému zaznamenanému ani žádnému komplementárnímu stavu.

V praxi se Hopfieldova síť příliš nevyužívá, právě z důvodů možné existence falešných rovnovážných stavů. Dá se však využít jako rekonstruktor zašuměných dat či pro řešení optimalizačních úloh (např. hledání nejkratší cesty). Princip: Optimalizační úloha se popíše vhodnou funkcí, která se převede do tvaru výpočetní energie sítě. Hopfieldova síť pak samovolně hledá minimum této funkce. Existují i sítě pracující v čase spojitě, ale při jejich analýze je třeba řešit nelineární diferenciální rovnice.

\subsection{Samoorganizující se sítě.}
Samy se snaží objevit zákonitosti a souvislosti ve vstupních datech (tzv. proces samoorganizace), tzn. během učení (bez učitele) se snaží nastavit své váhy a prahy tak, aby na podobné vstupy reagovaly podobnými výstupy $ \to $ dochází k tzv. shlukování vstupních dat. Míra podobnosti se obvykle posuzuje pomocí Eukleidovské vzdálenosti
\begin{equation}
||a-b|| = \sqrt{\displaystyle{\sum_{i=1}^n} (a_i-b_i)^2}
\end{equation}
Značení:
\begin{description}
\item[$ S_m(k) $] ... množina vektorů, které jsou zahrnuty ve shluku $ S_m $ před přidáním vektoru $ x $
\item[$ w_m(k) $] ... centroid shluku $ S_m $ před přidáním vektoru $ x $
\item[$ P_m(k) $] ... počet vektorů ve shluku $ S_m $ před přidáním vektoru $ x $
\item[$ S_m(k+1) $] ... množina vektorů, které jsou zahrnuty ve shluku $ S_m $ po přidání vektoru $ x $, tzn. $ S_m(k+1) = S_m(k) \cup x $
\item[$ w_m(k+1) $] ... centroid shluku $ S_m $ po přidání vektoru $ x $
\item[$ P_m(k+1) $] ... počet vektorů ve shluku $ S_m $ po přidání vektoru $ x $, tzn. $ P_m(k+1) = P_m(k) + 1 $
\end{description}
Předpokládejme, že máme množinu $ P $ vektorů dimenze $ n $: $ \{x_1, x_2, ..., x_P\} $, které chceme rozdělit do $ R $ shluků $ S_1, S_2, ..., S_R $, přičemž každý shluk je reprezentován svým centroidem $ w_1, w_2, ..., w_R $, pro které platí:
\begin{equation}
w_r = \frac{1}{P_r} \displaystyle{\sum_{x_j \in S_r}} x_j, \qquad r=1,...,R
\end{equation}
kde $ P_r $ je počet vektorů ve shluku $ S_r $. Vektor $ x $ chceme zařadit do jednoho z $ R $ shluků $ \to $ zařadíme ho do toho shluku $ S_m $, k jehož centroidu je $ x $ nejblíže:
\begin{equation}
||x-w_m|| = \underset{r=1,...,R}{\mathrm{min}} \, ||x-w_r||
\end{equation}
Po přidání vektoru $ x $ do shluku $ S_m $ se změní poloha centroidu $ w_m $, centroidy ostatních shluků zůstávají beze změny.

\subsubsection*{Kohonenova síť}
Určena pro shlukování vstupních vektorů dimenze $ n $ do $ R $ shluků. Je to jednovrstvá dopředná síť s $ R $ výstupy, $ n $ vstupy a nulovým prahovým vektorem. Každý řádek váhové matice je normalizován tak, že má velikost $ 1 $, tzn., že pro i-tý řádek váhové matice platí:
\begin{equation}
||w_i|| = \sqrt{\displaystyle{\sum_j} w_{ij}^2} = 1
\end{equation}

Síť se učí bez učitele. Učení probíhá na základě tzv. \textit{pravidla vítěze} ("vítěz bere vše") $ \to $ na základě vstupního vektoru $ x $ se změní váhy m-tého neuronu podle vztahu:
\begin{equation}
\hat{w}_m(k+1) = w_m(k) + c \cdot (x^T - w_m(k))
\end{equation}
kde $ c \in \langle 0.1, 0.7 \rangle $ je konstanta učení a m-tý neuron je tzv. \textit{vítěz}, pro kterého platí:
\begin{equation}
w_m \cdot x = \underset{r=1,...,R}{\mathrm{max}} \, w_r \cdot x
\end{equation}
To znamená, že se mění váhy neuronů s nejvyšší aktivační hodnotou, váhy ostatních neuronů se nemění. Po změně vah je třeba váhový vektor m-tého neuronu normalizovat, tzn.
\begin{equation}
w_m = \frac{\hat{w_m}}{||\hat{w_m}||}
\end{equation}
Váhová matice $ W(0) $ se inicializuje náhodnými čísly, poté je třeba provést normalizaci každého řádku.

Vlastnosti Kohonenovy sítě:
\begin{itemize}
\item Váhy neuronů se při trénování blíží do středu shluků, které reprezentují.
\item Síť má pouze jednu vrstvu $ \to $ lze s ní najít pouze lineárně oddělitelné shluky.
\item I v případě lineárně oddělitelných shluků nemusí být tyto shluky vždy nalezeny.
\item Váhy měnící se podle výše zmíněného vztahu při konstantní hodnotě $ c $ nekonvergují ke konstantním hodnotám. Proto se často využívá proměnlivá konstanta učení $ c(t) $, která má po určitou dobu $ t_p $ (např. 1000 kroků) konstantní hodnotu $ c_0 $, a pak se snižuje, např. dle vztahu:
\begin{equation}
c(t) = c_0 \cdot e^{-\frac{t-t_p}{t_q}}, \qquad t > t_p
\end{equation}
kde $ t_q $ udává rychlost poklesu. Pokud předem neznáme počet shluků, předpokládáme, že shluků je velké množství. Během trénování pak některé neurony pravděpodobně nebudou měnit své váhy $ \to $ nemají význam a lze je vypustit.
\item Po natrénování sítě bude pro zadaný vstupní vektor největší hodnota na výstupu toho neuronu, jehož váhy reprezentují centroid shluku, ke kterému má daný vstupní vektor nejblíže.
\end{itemize}
Využití: hledání shluků ve vstupních datech; vektorová kvantizace.

\subsubsection*{Kohonenova mapa (feature map)}
Jedná se o speciální případ Kohonenovy sítě. Neurony jsou uspořádány tak, že tvoří jednorozměrné či dvojrozměrné pole. Učení probíhá bez učitele dle pravidla vítěze. Mění se ovšem nejen váhy vítězného neuronu, ale i váhy neuronů okolních. Pro změnu vah platí:
\begin{equation}
w_i(k+1) = w_i(k) + c \cdot (x^T - w_i(k)), \qquad i \in N_m(k)
\end{equation}
\begin{description}
\item[$ w_i(k) $] ... i-tý řádek matice $ W $ (není normalizovaný)
\item[$ N_m(k) $] ... okolí m-tého (vítězného) neuronu v čase k
\end{description}
Pro vítězný neuron s indexem $ m $ přitom platí:
\begin{equation}
||x-w_m^T|| = \underset{i=1,...,R}{\mathrm{min}} \, ||x-w_i^T||
\end{equation}
Řádky váhové matice se nenormalizují! V procesu trénování sítě se velikost okolí zmenšuje. Na začátku trénování se velikost okolí obvykle volí tak, že zahrnuje všechny neurony sítě. Pak se velikost okolí lineárně snižuje až na nulu, kdy okolí obsahuje pouze vítězný neuron. Doba trénování se obvykle volí tak, aby doba, po kterou je velikost okolí rovna 0, byla přibližně 3-krát větší než doba, po kterou docházelo ke snižování velikosti okolí. Inicializace vah se provádí malými náhodnými čísly, obvykle z intervalu $ \langle -0.1, 0.1 \rangle $. Konstanta učení $ c $ se podobně jako u Kohonenovy sítě volí proměnlivá v čase. Kohonenova mapa se využívá pro redukci počtu příznaků pro klasifikaci či pro vizualizaci vektorů velké dimenze.

\subsection{Oblasti použití neuronových sítí.}
Obecně lze aplikace matematicky formulovat jako aproximaci funkce (tj. optimalizační úlohu):
\begin{equation}
y = f(x, parametry)
\end{equation}
\begin{itemize}
\item \textit{Klasifikátory}: úkolem je zařadit vstupní data do skupin (tříd) podle vzájemné podobnosti (nelineární sítě); Vstupem jsou jednotlivé klasifikované obrazy (tj. vektory příznaků), výstupem je informace o zařazení vstupního obrazu do určité třídy. Je-li $ R $ počet tříd, potom počet výstupů je sítě je obvykle roven buď $ R $ nebo $ log_2R $. Obecně platí, že počet výstupu sítě může být jakékoli číslo z intervalu $ \langle log_2R, R\rangle$. Většinou se ve všech vrstvách využívá některá ze sigmoidálních aktivačních funkcí, která se po natrénování ve výstupní vrstvě nahradí odpovídající binární aktivační funkcí.
\item \textit{Aproximátory funkcí (regresory)}: z několika zadaných (naměřených) hodnot je třeba sestavit funkční závislost (nelineární sítě); Jednotlivé vstupy nelineárních sítí představují nezávisle proměnné aproximované funkce, výstupy představují závisle proměnné. Pro aproximaci libovolné funkce postačují 2 až 3 skryté vrstvy neuronů:
\begin{itemize}
\item jedna vrstva rozdělí vstupní prostor nadrovinou (ve 2D přímkou)
\item dvě vrstvy jsou schopny oddělit konvexní oblast
\item tři vrstvy jsou schopny oddělit i nekonvexní oblasti
\end{itemize}
Počet skrytých vrstev a počet neuronů v nich má vliv na schopnost sítě zobecňovat (generalizovat) vstupní body trénovací množiny. Velké množství vrstev a neuronů může vést k přetrénování (overfitting) sítě, malé množství k nedotrénování (underfitting) sítě.
\item \textit{Paměti a rekonstruktory}: na základě předloženého vstupního obrazu je síť schopna "vybavit si" odpovídající výstupní obraz (asociativní paměť - nelineární sítě), popř. je schopna zrekonstruovat zašuměný vstupní obraz do původní podoby (Hopfieldova síť); Asociativní paměť je paměť adresovaná obsahem, adresou je klíčová hodnota ukládaná s informací. Vstupem sítě je vstupní obraz (tzv. klíč), kterým se přistupuje do paměti, výstupem sítě je příslušný asociovaný obraz. Zapamatovaná informace je v síti uložena v hodnotách vah a prahů a je rozprostřena po celé síti. Asociativní paměť je schopna vykonávat svou funkci dostatečně správně i při částečném poškození (například při odstranění části neuronů skryté vrstvy). U pamětí adresovaných adresou se obsah z poškozené části ztrácí úplně.
\item \textit{Optimalizace}: úkolem je minimalizovat určitou ztrátovou funkci, která je obvykle definována uživatelem (rozvrhování činností, hledání optimální cesty, apod.);
\item \textit{Shlukování a redukce příznaků}: objevování shluků ve vstupních datech a redukce počtu příznaků. Základní charakteristikou těchto sítí je tzv. samoorganizace (Hopfieldova síť).
\end{itemize}

\section{Zpracování digitalizovaného obrazu [ZDO]}

\begin{table}[H]
\centering
\begin{tabular}{p{4cm} p{12cm}}
\textit{vyučující:}             & Doc. Ing. Miloš Železný Ph.D. \\
								 & Ing. Petr Neduchal \\
\textit{ročník/semestr studia:} & 4.ročník/LS \\
\textit{datum zkoušky:}         & 13. 7. 2015 \\
\textit{hodnocení:}             & 1 \\
\textit{cíl předmětu (STAG):}   & \\
\multicolumn{2}{p{16cm}}{Porozumět principům zpracování digitalizovaného obrazu a počítačového vidění. Analyzovat vlastnosti obrazové informace a interpretovat tyto informace, navrhnout a vytvořit algoritmus pro zpracování obrazové informace s cílem rozpoznání objektů, jevů či vlastností scény v obraze obsažené.}
\end{tabular}
\end{table}

\subsection{Bodové jasové transformace.}

\subsection{Geometrické transformace.}

\subsection{Filtrace šumu.}

\subsection{Gradientní operátory.}

\subsection{Metody segmentace.}

\subsection{Matematická morfologie.}